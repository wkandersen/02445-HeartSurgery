{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import recall_score\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# data = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "# # # #make pd to np\n",
    "# X = data.to_numpy()\n",
    "# y = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# smote = SMOTE()\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_tensor.numpy(), y_tensor.numpy())\n",
    "# X_resampled = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "# y_resampled = torch.tensor(y_resampled, dtype=torch.float32)\n",
    "\n",
    "# # Create a dataset\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# # Split data into holdout set and remaining set\n",
    "# indices = np.arange(len(dataset))\n",
    "# np.random.seed(42)\n",
    "# np.random.shuffle(indices)\n",
    "# holdout_indices = indices[:500]\n",
    "# remaining_indices = indices[500:]\n",
    "\n",
    "# holdout_set = Subset(dataset, holdout_indices)\n",
    "# remaining_set = Subset(dataset, remaining_indices)\n",
    "\n",
    "# # Define the ANN model with L2 regularization\n",
    "# class DeepANNModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "#         super(DeepANNModel, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_layers = nn.ModuleList([\n",
    "#             nn.Linear(input_dim if i == 0 else hidden_dims[i-1], hidden_dim)\n",
    "#             for i, hidden_dim in enumerate(hidden_dims)\n",
    "#         ])\n",
    "#         self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.hidden_layers:\n",
    "#             x = self.relu(layer(x))\n",
    "#         x = self.sigmoid(self.output_layer(x))\n",
    "#         return x\n",
    "\n",
    "# input_dim = 575\n",
    "# hidden_dims = [128,64]\n",
    "# output_dim = 2\n",
    "\n",
    "# # Function to train and evaluate the model with L2 regularization\n",
    "# def train_and_evaluate_model(train_loader, val_loader, input_dim, hidden_dim, output_dim, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "#     model = DeepANNModel(input_dim, hidden_dim, output_dim)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for images, labels in train_loader:\n",
    "#             # Forward pass\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     accuracy = correct / total\n",
    "#     return accuracy, model\n",
    "\n",
    "# # Outer 10-Fold Cross-Validation\n",
    "# outer_kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "# outer_accuracies = []\n",
    "# best_model_params = None\n",
    "# best_outer_accuracy = 0\n",
    "\n",
    "# for outer_train_index, outer_test_index in outer_kf.split(remaining_indices):\n",
    "#     outer_train_subset = Subset(remaining_set, outer_train_index)\n",
    "#     outer_test_subset = Subset(remaining_set, outer_test_index)\n",
    "    \n",
    "#     # Inner 10-Fold Cross-Validation\n",
    "#     inner_kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "#     inner_accuracies = []\n",
    "\n",
    "#     for inner_train_index, inner_val_index in inner_kf.split(outer_train_index):\n",
    "#         inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "#         inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "        \n",
    "#         inner_train_subset = Subset(remaining_set, inner_train_indices)\n",
    "#         inner_val_subset = Subset(remaining_set, inner_val_indices)\n",
    "        \n",
    "#         train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "#         val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "        \n",
    "#         accuracy, model = train_and_evaluate_model(train_loader, val_loader, input_dim, hidden_dims, output_dim)\n",
    "#         inner_accuracies.append(accuracy)\n",
    "#         print(f'Inner Fold Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#     mean_inner_accuracy = np.mean(inner_accuracies)\n",
    "#     print(f'Outer Fold Mean Inner Accuracy: {mean_inner_accuracy * 100:.2f}%')\n",
    "\n",
    "#     train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "#     test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "#     outer_accuracy, outer_model = train_and_evaluate_model(train_loader, test_loader, input_dim, hidden_dims, output_dim)\n",
    "#     outer_accuracies.append(outer_accuracy)\n",
    "#     print(f'Outer Fold Accuracy: {outer_accuracy * 100:.2f}%')\n",
    "\n",
    "#     if outer_accuracy > best_outer_accuracy:\n",
    "#         best_outer_accuracy = outer_accuracy\n",
    "#         best_model_params = outer_model.state_dict()\n",
    "\n",
    "# print(f'Mean Outer Accuracy: {np.mean(outer_accuracies) * 100:.2f}%')\n",
    "# print('Best model parameters:', best_model_params)\n",
    "\n",
    "# # Evaluate the final model on the holdout set\n",
    "# final_model = DeepANNModel(input_dim, hidden_dims, output_dim)\n",
    "# final_model.load_state_dict(best_model_params)\n",
    "# train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "# holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# import torch\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# def evaluate_holdout(model, holdout_loader):\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in holdout_loader:\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             all_preds.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     precision = precision_score(all_labels, all_preds, average='macro')\n",
    "#     recall = recall_score(all_labels, all_preds, average='macro')\n",
    "#     return precision, recall\n",
    "\n",
    "# # Example usage\n",
    "# holdout_precision, holdout_recall = evaluate_holdout(final_model, holdout_loader)\n",
    "# print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%')\n",
    "# print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# def evaluate_roc_auc(model, holdout_loader):\n",
    "#     model.eval()\n",
    "#     all_probs = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in holdout_loader:\n",
    "#             outputs = model(images)\n",
    "#             probs = torch.softmax(outputs, dim=1)[:, 1]  # Assuming binary classification\n",
    "#             all_probs.extend(probs.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "#     auc_score = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {auc_score:.2f})')\n",
    "#     plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.0])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return auc_score\n",
    "\n",
    "# # Example usage\n",
    "# auc_score = evaluate_roc_auc(final_model, holdout_loader)\n",
    "# print(f'AUC Score: {auc_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, recall_score, roc_curve, auc\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# data = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "# # Convert data to numpy arrays\n",
    "# X = data.to_numpy()\n",
    "# y = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "#   # Note: use float32 for BCEWithLogitsLoss\n",
    "\n",
    "# # Create a dataset\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# # Split data into holdout set and remaining set\n",
    "# indices = np.arange(len(dataset))\n",
    "# np.random.seed(42)\n",
    "# np.random.shuffle(indices)\n",
    "# holdout_indices = indices[:500]\n",
    "# remaining_indices = indices[500:]\n",
    "\n",
    "# holdout_set = Subset(dataset, holdout_indices)\n",
    "# remaining_set = Subset(dataset, remaining_indices)\n",
    "\n",
    "# # Define the deep ANN model with L2 regularization\n",
    "# class DeepANNModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims):\n",
    "#         super(DeepANNModel, self).__init__()\n",
    "#         self.hidden_layers = nn.ModuleList([\n",
    "#             nn.Linear(input_dim if i == 0 else hidden_dims[i-1], hidden_dim)\n",
    "#             for i, hidden_dim in enumerate(hidden_dims)\n",
    "#         ])\n",
    "#         self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.hidden_layers:\n",
    "#             x = self.relu(layer(x))\n",
    "#         x = self.output_layer(x)\n",
    "#         return x\n",
    "\n",
    "# input_dim = 575\n",
    "# hidden_dims = [128, 64]\n",
    "\n",
    "# # Function to train and evaluate the model with L2 regularization\n",
    "# def train_and_evaluate_model(train_loader, val_loader, input_dim, hidden_dims, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "#     model = DeepANNModel(input_dim, hidden_dims)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for images, labels in train_loader:\n",
    "#             # Forward pass\n",
    "#             outputs = model(images).squeeze()\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     all_outputs = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             outputs = model(images).squeeze()\n",
    "#             predicted = torch.round(torch.sigmoid(outputs))\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#             all_outputs.extend(outputs.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     accuracy = correct / total\n",
    "#     return accuracy, model\n",
    "\n",
    "# # Outer 10-Fold Cross-Validation\n",
    "# outer_kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# outer_accuracies = []\n",
    "# best_model_params = None\n",
    "# best_outer_accuracy = 0\n",
    "\n",
    "# for outer_train_index, outer_test_index in outer_kf.split(remaining_indices):\n",
    "#     outer_train_subset = Subset(remaining_set, outer_train_index)\n",
    "#     outer_test_subset = Subset(remaining_set, outer_test_index)\n",
    "    \n",
    "#     # Inner 10-Fold Cross-Validation\n",
    "#     inner_kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     inner_accuracies = []\n",
    "\n",
    "#     for inner_train_index, inner_val_index in inner_kf.split(outer_train_index):\n",
    "#         inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "#         inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "        \n",
    "#         inner_train_subset = Subset(remaining_set, inner_train_indices)\n",
    "#         inner_val_subset = Subset(remaining_set, inner_val_indices)\n",
    "        \n",
    "#         train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "#         val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "        \n",
    "#         accuracy, model = train_and_evaluate_model(train_loader, val_loader, input_dim, hidden_dims)\n",
    "#         inner_accuracies.append(accuracy)\n",
    "#         print(f'Inner Fold Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#     mean_inner_accuracy = np.mean(inner_accuracies)\n",
    "#     print(f'Outer Fold Mean Inner Accuracy: {mean_inner_accuracy * 100:.2f}%')\n",
    "\n",
    "#     train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "#     test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "#     outer_accuracy, outer_model = train_and_evaluate_model(train_loader, test_loader, input_dim, hidden_dims)\n",
    "#     outer_accuracies.append(outer_accuracy)\n",
    "#     print(f'Outer Fold Accuracy: {outer_accuracy * 100:.2f}%')\n",
    "\n",
    "#     if outer_accuracy > best_outer_accuracy:\n",
    "#         best_outer_accuracy = outer_accuracy\n",
    "#         best_model_params = outer_model.state_dict()\n",
    "\n",
    "# print(f'Mean Outer Accuracy: {np.mean(outer_accuracies) * 100:.2f}%')\n",
    "# print('Best model parameters:', best_model_params)\n",
    "\n",
    "# # Evaluate the final model on the holdout set\n",
    "# final_model = DeepANNModel(input_dim, hidden_dims)\n",
    "# final_model.load_state_dict(best_model_params)\n",
    "# train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "# holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# # Evaluate the final model on the holdout set\n",
    "# final_model.eval()\n",
    "# holdout_outputs = []\n",
    "# holdout_targets = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in holdout_loader:\n",
    "#         outputs = final_model(inputs).squeeze()\n",
    "#         probabilities = torch.sigmoid(outputs)  # Get the probabilities for the positive class\n",
    "#         holdout_outputs.extend(probabilities.numpy())\n",
    "#         holdout_targets.extend(labels.numpy())\n",
    "\n",
    "# # Convert holdout outputs to binary predictions\n",
    "# holdout_predictions = np.round(holdout_outputs)\n",
    "\n",
    "# holdout_accuracy = accuracy_score(holdout_targets, holdout_predictions)\n",
    "# holdout_recall = recall_score(holdout_targets, holdout_predictions)\n",
    "# print(f'Holdout Set Accuracy: {holdout_accuracy * 100:.2f}%')\n",
    "# print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')\n",
    "\n",
    "# # Calculate false positive rate, true positive rate, and thresholds\n",
    "# fpr, tpr, thresholds = roc_curve(holdout_targets, holdout_outputs)\n",
    "\n",
    "# # Calculate AUC\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brug den her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "data1 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data2 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data3 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data4 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data5 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data6 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "base = data1\n",
    "phase1 = pd.concat([data1, data2], axis = 1)\n",
    "phase2 = pd.concat([data1, data2, data3], axis = 1)\n",
    "phase3 = pd.concat([data1, data2, data3, data4], axis = 1)\n",
    "phase4 = pd.concat([data1, data2, data3, data4, data5], axis = 1)\n",
    "phase5 = pd.concat([data1, data2, data3, data4, data5, data6], axis = 1)\n",
    "y_pred = \n",
    "data_list = [base, phase1, phase2, phase3, phase4, phase5]\n",
    "\n",
    "preds_log = []\n",
    "models_log = []\n",
    "holdout_log = []\n",
    "true_log = []\n",
    "\n",
    "for i in tqdm(range(len(data_list))):\n",
    "    data = data_list[i]\n",
    "    # Load your dataset\n",
    "    data = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "    # Preprocess your data\n",
    "    X = data.to_numpy()\n",
    "    y = y_pred.to_numpy()\n",
    "\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long).flatten()\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Split data into holdout set and remaining set\n",
    "    train_indices, holdout_indices = train_test_split(np.arange(len(dataset)), test_size=500, random_state=42, stratify=y_tensor.numpy())\n",
    "\n",
    "    holdout_set = Subset(dataset, holdout_indices)\n",
    "    remaining_set = Subset(dataset, train_indices)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_counts = np.bincount(y_tensor.numpy())\n",
    "    class_weights = 1.0 / class_counts\n",
    "    weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    print(f'Class Weights: {weights}')\n",
    "\n",
    "    # Function to get target tensor from Subset\n",
    "    def get_targets(subset):\n",
    "        return subset.dataset.tensors[1][subset.indices]\n",
    "\n",
    "    # Get target tensors for holdout and remaining sets\n",
    "    holdout_targets = get_targets(holdout_set)\n",
    "    remaining_targets = get_targets(remaining_set)\n",
    "\n",
    "    # Print class distributions\n",
    "    print(f\"Class distribution in holdout set: {np.unique(holdout_targets.numpy(), return_counts=True)}\")\n",
    "    print(f\"Class distribution in remaining set: {np.unique(remaining_targets.numpy(), return_counts=True)}\")\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    weight_decay_values = np.logspace(-5, 1, 7)\n",
    "    lr_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "    # Define the ANN model\n",
    "    class DeepANNModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "            super(DeepANNModel, self).__init__()\n",
    "            self.input_dim = input_dim\n",
    "            self.hidden_layers = nn.ModuleList([\n",
    "                nn.Linear(input_dim if i == 0 else hidden_dims[i-1], hidden_dim)\n",
    "                for i, hidden_dim in enumerate(hidden_dims)\n",
    "            ])\n",
    "            self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            for layer in self.hidden_layers:\n",
    "                x = self.relu(layer(x))\n",
    "            x = self.output_layer(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "\n",
    "    # Function to train and evaluate the model with L2 regularization\n",
    "    def train_and_evaluate_model(train_loader, val_loader, input_dim, hidden_dims, output_dim, num_epochs=10, lr=0.01, weight_decay=1e-5, class_weights=weights):\n",
    "        model = DeepANNModel(input_dim, hidden_dims, output_dim)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for images, labels in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro',zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        return f1, model, accuracy, all_preds, all_labels, precision, recall\n",
    "\n",
    "\n",
    "    # Outer 5-Fold Cross-Validation\n",
    "    outer_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # Function for grid search over different weight_decay values\n",
    "    def grid_search_weight_decay(train_set, outer_kf, input_dim, hidden_dims, output_dim, num_epochs=10, lr=lr_values, weight_decay_values=weight_decay_values):\n",
    "        best_weight_decay = None\n",
    "        best_outer_f1 = 0\n",
    "\n",
    "        y_remaining = y_tensor[train_indices].numpy()\n",
    "        for lr in lr_values:\n",
    "            for weight_decay in weight_decay_values:\n",
    "                print(f'Grid search for weight_decay={weight_decay} and lr={lr}')\n",
    "                outer_f1_scores = []\n",
    "\n",
    "                for outer_train_index, outer_test_index in outer_kf.split(np.zeros(len(train_set)), y_remaining):\n",
    "                    outer_train_subset = Subset(train_set, outer_train_index)\n",
    "                    outer_test_subset = Subset(train_set, outer_test_index)\n",
    "                    inner_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                    inner_f1_scores = []\n",
    "                    \n",
    "                    for inner_train_index, inner_val_index in inner_kf.split(np.zeros(len(outer_train_index)), y_remaining[outer_train_index]):\n",
    "                        inner_train_subset = Subset(outer_train_subset, inner_train_index)\n",
    "                        inner_val_subset = Subset(outer_train_subset, inner_val_index)\n",
    "                        \n",
    "                        train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "                        val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "                        \n",
    "                        f1, _, _, _, _, _, _ = train_and_evaluate_model(train_loader, val_loader, input_dim, hidden_dims, output_dim, num_epochs, lr, weight_decay)\n",
    "                        inner_f1_scores.append(f1)\n",
    "\n",
    "                    # Train on outer train subset and evaluate on outer test subset\n",
    "                    train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "                    test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "                    \n",
    "                    outer_f1, _, _, _, _, _, _ = train_and_evaluate_model(train_loader, test_loader, input_dim, hidden_dims, output_dim, num_epochs, lr, weight_decay)\n",
    "                    outer_f1_scores.append(outer_f1)\n",
    "\n",
    "                    mean_outer_f1 = np.mean(outer_f1_scores)\n",
    "                    print(f'Weight Decay: {weight_decay}, Learning rate: {lr} Mean Outer F1 Score: {mean_outer_f1:.4f}')\n",
    "                    \n",
    "                    if mean_outer_f1 > best_outer_f1:\n",
    "                        best_outer_f1 = mean_outer_f1\n",
    "                        best_weight_decay = weight_decay\n",
    "                        best_learning_rate = lr\n",
    "                \n",
    "            return best_weight_decay, best_learning_rate\n",
    "\n",
    "\n",
    "\n",
    "    # Perform grid search over different weight_decay values\n",
    "    best_weight_decay, best_learning_rate = grid_search_weight_decay(remaining_set, outer_kf, input_dim, hidden_dims=[128, 64], output_dim=2, num_epochs=10, lr=0.01, weight_decay_values=weight_decay_values)\n",
    "\n",
    "    print(f'Best weight_decay found: {best_weight_decay}')\n",
    "    print(f'Best learning rate found: {best_learning_rate}')\n",
    "\n",
    "    # Evaluate the final model on the holdout set with the best weight_decay\n",
    "    final_model = DeepANNModel(input_dim, hidden_dims=[128, 64], output_dim=2)\n",
    "    train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "    holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    f1, model, holdout_acc, holdout_preds, holdout_all_labels, holdout_precision, holdout_recall = train_and_evaluate_model(train_loader, holdout_loader, input_dim, hidden_dims=[128, 64], output_dim=2, num_epochs=10, lr=best_learning_rate, weight_decay=best_weight_decay, class_weights=weights)\n",
    "\n",
    "    models_log.append(final_model)\n",
    "    holdout_log.append(holdout_loader)\n",
    "    true_log.append(holdout_all_labels)\n",
    "    preds_log.append(holdout_preds)\n",
    "\n",
    "    print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%')\n",
    "    print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%')\n",
    "    print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')\n",
    "    print(f'Holdout Set F1 Score: {f1 * 100:.2f}%')\n",
    "\n",
    "    with open('ANN_output.txt', 'a') as file:\n",
    "        print(f\"Phase {i + 1}\", file=file)\n",
    "        print(f'Best Weight Decay: {best_weight_decay}', file=file)\n",
    "        print(f'Best Learning Rate: {best_learning_rate}', file=file)\n",
    "        print(f'Final Model F1 Score on Holdout Set: {f1:.4f}', file=file)\n",
    "        print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%', file=file)\n",
    "        print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%', file=file)\n",
    "        print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%', file=file)\n",
    "        print('\\n', file=file)\n",
    "        print('Output written to output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def evaluate_roc_auc(model, holdout_loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in holdout_loader:\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # Assuming binary classification\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "# Example usage\n",
    "auc_score = evaluate_roc_auc(final_model, holdout_loader)\n",
    "print(f'AUC Score: {auc_score:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
