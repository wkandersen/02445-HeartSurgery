{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #logistic regression\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the data\n",
    "# data = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "# #make pd to np\n",
    "# X = data.to_numpy()\n",
    "# y = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# # Generate a synthetic dataset\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# # Create a dataset\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# # Split data into holdout set and remaining set\n",
    "# indices = np.arange(len(dataset))\n",
    "# np.random.seed(42)\n",
    "# np.random.shuffle(indices)\n",
    "# holdout_indices = indices[:500]\n",
    "# remaining_indices = indices[500:]\n",
    "\n",
    "# holdout_set = Subset(dataset, holdout_indices)\n",
    "# remaining_set = Subset(dataset, remaining_indices)\n",
    "\n",
    "# class_weights = torch.tensor([1/0.98, 1/0.02])\n",
    "\n",
    "# # Define the logistic regression model with L2 regularization\n",
    "# class LogisticRegressionModel(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(LogisticRegressionModel, self).__init__()\n",
    "#         self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = torch.sigmoid(self.linear(x))\n",
    "#         return out\n",
    "\n",
    "# input_dim = 575\n",
    "\n",
    "# # Function to train and evaluate the model with L2 regularization\n",
    "# def train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "#     model = LogisticRegressionModel(input_dim)\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for images, labels in train_loader:\n",
    "#             # Forward pass\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             outputs = model(images)\n",
    "#             predicted = (outputs > 0.5).float()\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     accuracy = correct / total\n",
    "#     return accuracy, model\n",
    "\n",
    "# # Outer 10-Fold Cross-Validation\n",
    "# outer_kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# outer_accuracies = []\n",
    "# best_model_params = None\n",
    "# best_outer_accuracy = 0\n",
    "\n",
    "# y_remaining = y_tensor[remaining_indices]\n",
    "\n",
    "# for outer_train_index, outer_test_index in outer_kf.split(remaining_indices):\n",
    "#     outer_train_subset = Subset(remaining_set, outer_train_index)\n",
    "#     outer_test_subset = Subset(remaining_set, outer_test_index)\n",
    "    \n",
    "#     # Inner 10-Fold Cross-Validation\n",
    "#     inner_kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#     inner_accuracies = []\n",
    "\n",
    "#     for inner_train_index, inner_val_index in inner_kf.split(outer_train_index):\n",
    "#         inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "#         inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "        \n",
    "#         inner_train_subset = Subset(remaining_set, inner_train_indices)\n",
    "#         inner_val_subset = Subset(remaining_set, inner_val_indices)\n",
    "        \n",
    "#         train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "#         val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "        \n",
    "#         accuracy, model = train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5)\n",
    "#         inner_accuracies.append(accuracy)\n",
    "#         print(f'Inner Fold Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#     mean_inner_accuracy = np.mean(inner_accuracies)\n",
    "#     print(f'Outer Fold Mean Inner Accuracy: {mean_inner_accuracy * 100:.2f}%')\n",
    "\n",
    "#     train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "#     test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "#     outer_accuracy, outer_model = train_and_evaluate_model(train_loader, test_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5)\n",
    "#     outer_accuracies.append(outer_accuracy)\n",
    "#     print(f'Outer Fold Accuracy: {outer_accuracy * 100:.2f}%')\n",
    "\n",
    "#     if outer_accuracy > best_outer_accuracy:\n",
    "#         best_outer_accuracy = outer_accuracy\n",
    "#         best_model_params = outer_model.state_dict()\n",
    "\n",
    "# print(f'Mean Outer Accuracy: {np.mean(outer_accuracies) * 100:.2f}%')\n",
    "# print('Best model parameters:', best_model_params)\n",
    "\n",
    "# # Evaluate the final model on the holdout set\n",
    "# final_model = LogisticRegressionModel(input_dim)\n",
    "# final_model.load_state_dict(best_model_params)\n",
    "# holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# # Evaluate on holdout set\n",
    "# final_model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in holdout_loader:\n",
    "#         outputs = final_model(images)\n",
    "#         predicted = (outputs > 0.5).float()\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# holdout_accuracy = correct / total\n",
    "# print(f'Holdout Set Accuracy: {holdout_accuracy * 100:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc, recall_score\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Function to calculate AUC-ROC curve and recall score\n",
    "# def plot_roc_curve_and_recall(model, loader, title=\"ROC Curve and Recall Score\"):\n",
    "#     model.eval()\n",
    "#     y_true = []\n",
    "#     y_scores = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in loader:\n",
    "#             outputs = model(images)\n",
    "#             y_true.extend(labels.numpy())\n",
    "#             y_scores.extend(outputs.numpy())\n",
    "\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.figure()\n",
    "#     lw = 2\n",
    "#     plt.plot(fpr, tpr, color='darkorange',\n",
    "#              lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title(title)\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "\n",
    "#     # Calculate Recall Score\n",
    "#     y_pred = (np.array(y_scores) > 0.5).astype(int)\n",
    "#     recall = recall_score(y_true, y_pred)\n",
    "#     print(\"Recall Score:\", recall)\n",
    "\n",
    "# # Define DataLoader for holdout set\n",
    "# holdout_loader = holdout_log[1]\n",
    "\n",
    "# # Load best model parameters\n",
    "# final_model = models_log[1]\n",
    "\n",
    "# # Plot ROC curve and calculate recall score\n",
    "# plot_roc_curve_and_recall(final_model, holdout_loader)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # Load the data\n",
    "# data1 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "# data2 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "# data3 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "# data4 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "# data5 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "# data6 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "# base = data1\n",
    "# phase1 = pd.concat([data1, data2], axis = 1)\n",
    "# phase2 = pd.concat([data1, data2, data3], axis = 1)\n",
    "# phase3 = pd.concat([data1, data2, data3, data4], axis = 1)\n",
    "# phase4 = pd.concat([data1, data2, data3, data4, data5], axis = 1)\n",
    "# phase5 = pd.concat([data1, data2, data3, data4, data5, data6], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# data = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "# # Convert to numpy array and generate synthetic labels for demonstration\n",
    "# X = data.to_numpy()\n",
    "# y = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# # Standardize the data\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# # Create a dataset\n",
    "# dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# # Split data into holdout set and remaining set\n",
    "# train_indices, holdout_indices = train_test_split(np.arange(len(dataset)), test_size=500, random_state=42, stratify=y_tensor.numpy())\n",
    "\n",
    "# holdout_set = Subset(dataset, holdout_indices)\n",
    "# remaining_set = Subset(dataset, train_indices)\n",
    "\n",
    "# y_1_percent = np.count_nonzero(y_tensor) / len(y_tensor)\n",
    "# y_0_percent = 1 - y_1_percent\n",
    "# print(f\"Class 1 percentage: {y_1_percent * 100:.2f}%\")\n",
    "# print(f\"Class 0 percentage: {y_0_percent * 100:.2f}%\")\n",
    "# # Define class weights for BCEWithLogitsLoss\n",
    "# class_weights = torch.tensor([1/y_0_percent, 1/y_1_percent])\n",
    "\n",
    "# # Define the logistic regression model with L2 regularization\n",
    "# class LogisticRegressionModel(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(LogisticRegressionModel, self).__init__()\n",
    "#         self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = torch.sigmoid(self.linear(x))\n",
    "#         return out\n",
    "\n",
    "# input_dim = data.shape[1]\n",
    "\n",
    "# # Function to get target tensor from Subset\n",
    "# def get_targets(subset):\n",
    "#     return subset.dataset.tensors[1][subset.indices]\n",
    "\n",
    "# # Get target tensors for holdout and remaining sets\n",
    "# holdout_targets = get_targets(holdout_set)\n",
    "# remaining_targets = get_targets(remaining_set)\n",
    "\n",
    "# # Print class distributions\n",
    "# print(f\"Class distribution in holdout set: {np.unique(holdout_targets.numpy(), return_counts=True)}\")\n",
    "# print(f\"Class distribution in remaining set: {np.unique(remaining_targets.numpy(), return_counts=True)}\")\n",
    "\n",
    "# weight_decay_values = np.logspace(-5, 1, 7)  # for example, try values from 1e-5 to 10\n",
    "\n",
    "# # Function to train and evaluate the model with L2 regularization\n",
    "# def train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "#     model = LogisticRegressionModel(input_dim)\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         for inputs, labels in train_loader:\n",
    "#             # Forward pass\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "#             # Backward pass and optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     # Evaluate on validation set\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             predicted = (outputs > 0.5).float().squeeze()\n",
    "#             all_preds.extend(predicted.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     f1 = f1_score(all_labels, all_preds)\n",
    "#     return f1, model\n",
    "\n",
    "# outer_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# # Function to perform grid search over weight_decay values\n",
    "# def grid_search(train_set, outer_kf, weight_decays, input_dim, num_epochs=10, lr=0.01):\n",
    "#     best_weight_decay = None\n",
    "#     best_outer_f1 = 0\n",
    "#     y_remaining = y_tensor[train_indices].numpy()\n",
    "\n",
    "#     for weight_decay in weight_decays:\n",
    "#         outer_f1_scores = []\n",
    "        \n",
    "#         for outer_train_index, outer_test_index in outer_kf.split(np.zeros(len(train_set)), y_remaining):\n",
    "            \n",
    "#             outer_train_subset = Subset(train_set, outer_train_index)\n",
    "#             outer_test_subset = Subset(train_set, outer_test_index)\n",
    "            \n",
    "#             # Inner 5-Fold Cross-Validation\n",
    "#             inner_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#             inner_f1_scores = []\n",
    "            \n",
    "#             for inner_train_index, inner_val_index in inner_kf.split(np.zeros(len(outer_train_index)), y_remaining[outer_train_index]):\n",
    "                \n",
    "#                 inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "#                 inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "                \n",
    "#                 inner_train_subset = Subset(train_set, inner_train_indices)\n",
    "#                 inner_val_subset = Subset(train_set, inner_val_indices)\n",
    "                \n",
    "#                 train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "#                 val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "                \n",
    "#                 f1, _ = train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs, lr, weight_decay)\n",
    "#                 inner_f1_scores.append(f1)\n",
    "                        \n",
    "#             # Train on outer train subset and evaluate on outer test subset\n",
    "#             train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "#             test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "            \n",
    "#             outer_f1, _ = train_and_evaluate_model(train_loader, test_loader, input_dim, num_epochs, lr, weight_decay)\n",
    "#             outer_f1_scores.append(outer_f1)\n",
    "        \n",
    "#         mean_outer_f1 = np.mean(outer_f1_scores)\n",
    "#         print(f'Weight Decay: {weight_decay}, Mean Outer F1 Score: {mean_outer_f1:.4f}')\n",
    "        \n",
    "#         if mean_outer_f1 > best_outer_f1:\n",
    "#             best_outer_f1 = mean_outer_f1\n",
    "#             best_weight_decay = weight_decay\n",
    "    \n",
    "#     return best_weight_decay\n",
    "\n",
    "\n",
    "# # Perform grid search over weight_decay values\n",
    "\n",
    "# best_weight_decay = grid_search(remaining_set, outer_kf, weight_decay_values, input_dim, num_epochs=10, lr=0.01)\n",
    "\n",
    "# print(f'Best Weight Decay: {best_weight_decay}')\n",
    "\n",
    "# # Evaluate the final model on the holdout set\n",
    "# final_model = LogisticRegressionModel(input_dim)\n",
    "# train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "# holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# # Train the final model with the best weight decay found\n",
    "# holdout_f1, _ = train_and_evaluate_model(train_loader, holdout_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=best_weight_decay)\n",
    "# print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}')\n",
    "\n",
    "# # Evaluate precision and recall on holdout set\n",
    "# final_model.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in holdout_loader:\n",
    "#         outputs = final_model(inputs)\n",
    "#         predicted = (outputs > 0.5).float()\n",
    "        \n",
    "#         all_preds.extend(predicted.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# holdout_precision = precision_score(all_labels, all_preds)\n",
    "# holdout_recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "# print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%')\n",
    "# print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')\n",
    "\n",
    "# # Open the file in write mode\n",
    "# with open('output.txt', 'w') as file:\n",
    "#     print(f'Best Weight Decay: {best_weight_decay}', file=file)\n",
    "#     print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}', file=file)\n",
    "#     print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%', file=file)\n",
    "#     print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%', file=file)\n",
    "#     print('Output written to output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\mathtt{\\text{[1.e-05 1.e-04 1.e-03 1.e-02 1.e-01 1.e+00 1.e+01]}}\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "print_latex(np.logspace(-5, 1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 percentage: 49.39%\n",
      "Class 0 percentage: 50.61%\n",
      "Class distribution in holdout set: (array([0., 1.], dtype=float32), array([253, 247]))\n",
      "Class distribution in remaining set: (array([0., 1.], dtype=float32), array([1359, 1326]))\n",
      "Weight Decay: 1e-05, Mean Outer F1 Score: 0.5044\n",
      "Weight Decay: 0.0001, Mean Outer F1 Score: 0.5180\n",
      "Weight Decay: 0.001, Mean Outer F1 Score: 0.5032\n",
      "Weight Decay: 0.01, Mean Outer F1 Score: 0.5028\n",
      "Weight Decay: 0.1, Mean Outer F1 Score: 0.5195\n",
      "Weight Decay: 1.0, Mean Outer F1 Score: 0.5296\n",
      "Weight Decay: 10.0, Mean Outer F1 Score: 0.5200\n",
      "Best Weight Decay: 1.0\n",
      "Final Model F1 Score on Holdout Set: 0.5481\n",
      "Holdout Set Accuracy: 51.20%\n",
      "Holdout Set Precision: 48.05%\n",
      "Holdout Set Recall: 49.80%\n",
      "Output written to output.txt\n",
      "Class 1 percentage: 49.39%\n",
      "Class 0 percentage: 50.61%\n",
      "Class distribution in holdout set: (array([0., 1.], dtype=float32), array([253, 247]))\n",
      "Class distribution in remaining set: (array([0., 1.], dtype=float32), array([1359, 1326]))\n",
      "Weight Decay: 1e-05, Mean Outer F1 Score: 0.5077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 177\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_weight_decay\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Perform grid search over weight_decay values\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m best_weight_decay \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_kf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest Weight Decay: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_weight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Evaluate the final model on the holdout set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 155\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(train_set, outer_kf, weight_decays, input_dim, num_epochs, lr)\u001b[0m\n\u001b[1;32m    152\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(inner_train_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    153\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m DataLoader(inner_val_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 155\u001b[0m     f1, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     inner_f1_scores\u001b[38;5;241m.\u001b[39mappend(f1)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Train on outer train subset and evaluate on outer test subset\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(train_loader, val_loader, input_dim, num_epochs, lr, weight_decay)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n",
      "File \u001b[0;32m~/activelearning/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/activelearning/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/activelearning/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data1 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data2 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data3 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data4 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data5 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "data6 = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "base = data1\n",
    "phase1 = pd.concat([data1, data2], axis = 1)\n",
    "phase2 = pd.concat([data1, data2, data3], axis = 1)\n",
    "phase3 = pd.concat([data1, data2, data3, data4], axis = 1)\n",
    "phase4 = pd.concat([data1, data2, data3, data4, data5], axis = 1)\n",
    "phase5 = pd.concat([data1, data2, data3, data4, data5, data6], axis = 1)\n",
    "\n",
    "data_list = [base, phase1, phase2, phase3, phase4, phase5]\n",
    "preds_log = []\n",
    "models_log = []\n",
    "holdout_log = []\n",
    "true_log = []\n",
    "for i in range(len(data_list)):\n",
    "    data = data_list[i]\n",
    "    # Convert to numpy array and generate synthetic labels for demonstration\n",
    "    X = data.to_numpy()\n",
    "    np.random.seed(42)\n",
    "    y = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "    # Standardize the data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Split data into holdout set and remaining set\n",
    "    train_indices, holdout_indices = train_test_split(np.arange(len(dataset)), test_size=500, random_state=42, stratify=y_tensor.numpy())\n",
    "\n",
    "    holdout_set = Subset(dataset, holdout_indices)\n",
    "    remaining_set = Subset(dataset, train_indices)\n",
    "\n",
    "    y_1_percent = np.count_nonzero(y_tensor) / len(y_tensor)\n",
    "    y_0_percent = 1 - y_1_percent\n",
    "    print(f\"Class 1 percentage: {y_1_percent * 100:.2f}%\")\n",
    "    print(f\"Class 0 percentage: {y_0_percent * 100:.2f}%\")\n",
    "    # Define class weights for BCEWithLogitsLoss\n",
    "    class_weights = torch.tensor([1/y_0_percent, 1/y_1_percent])\n",
    "\n",
    "    # Define the logistic regression model with L2 regularization\n",
    "    class LogisticRegressionModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(LogisticRegressionModel, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = torch.sigmoid(self.linear(x))\n",
    "            return out\n",
    "\n",
    "    input_dim = data.shape[1]\n",
    "\n",
    "    # Function to get target tensor from Subset\n",
    "    def get_targets(subset):\n",
    "        return subset.dataset.tensors[1][subset.indices]\n",
    "\n",
    "    # Get target tensors for holdout and remaining sets\n",
    "    holdout_targets = get_targets(holdout_set)\n",
    "    remaining_targets = get_targets(remaining_set)\n",
    "\n",
    "    # Print class distributions\n",
    "    print(f\"Class distribution in holdout set: {np.unique(holdout_targets.numpy(), return_counts=True)}\")\n",
    "    print(f\"Class distribution in remaining set: {np.unique(remaining_targets.numpy(), return_counts=True)}\")\n",
    "\n",
    "    weight_decay_values = np.logspace(-5, 1, 7)  # for example, try values from 1e-5 to 10\n",
    "\n",
    "    # Function to train and evaluate the model with L2 regularization\n",
    "    def train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "        model = LogisticRegressionModel(input_dim)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float().squeeze()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        accuracy = correct / total\n",
    "        return f1, model, accuracy, all_preds, all_labels\n",
    "\n",
    "    outer_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # Function to perform grid search over weight_decay values\n",
    "    def grid_search(train_set, outer_kf, weight_decays, input_dim, num_epochs=10, lr=0.01):\n",
    "        best_weight_decay = None\n",
    "        best_outer_f1 = 0\n",
    "        y_remaining = y_tensor[train_indices].numpy()\n",
    "\n",
    "        for weight_decay in weight_decays:\n",
    "            outer_f1_scores = []\n",
    "            \n",
    "            for outer_train_index, outer_test_index in outer_kf.split(np.zeros(len(train_set)), y_remaining):\n",
    "                \n",
    "                outer_train_subset = Subset(train_set, outer_train_index)\n",
    "                outer_test_subset = Subset(train_set, outer_test_index)\n",
    "                \n",
    "                # Inner 5-Fold Cross-Validation\n",
    "                inner_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                inner_f1_scores = []\n",
    "                \n",
    "                for inner_train_index, inner_val_index in inner_kf.split(np.zeros(len(outer_train_index)), y_remaining[outer_train_index]):\n",
    "                    \n",
    "                    inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "                    inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "                    \n",
    "                    inner_train_subset = Subset(train_set, inner_train_indices)\n",
    "                    inner_val_subset = Subset(train_set, inner_val_indices)\n",
    "                    \n",
    "                    train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "                    val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "                    \n",
    "                    f1, _, _, _, _ = train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs, lr, weight_decay)\n",
    "                    inner_f1_scores.append(f1)\n",
    "                            \n",
    "                # Train on outer train subset and evaluate on outer test subset\n",
    "                train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "                test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "                \n",
    "                outer_f1, _, _, _, _ = train_and_evaluate_model(train_loader, test_loader, input_dim, num_epochs, lr, weight_decay)\n",
    "                outer_f1_scores.append(outer_f1)\n",
    "            \n",
    "            mean_outer_f1 = np.mean(outer_f1_scores)\n",
    "            print(f'Weight Decay: {weight_decay}, Mean Outer F1 Score: {mean_outer_f1:.4f}')\n",
    "            \n",
    "            if mean_outer_f1 > best_outer_f1:\n",
    "                best_outer_f1 = mean_outer_f1\n",
    "                best_weight_decay = weight_decay\n",
    "        \n",
    "        return best_weight_decay\n",
    "\n",
    "\n",
    "    # Perform grid search over weight_decay values\n",
    "\n",
    "    best_weight_decay = grid_search(remaining_set, outer_kf, weight_decay_values, input_dim, num_epochs=10, lr=0.01)\n",
    "\n",
    "    print(f'Best Weight Decay: {best_weight_decay}')\n",
    "\n",
    "    # Evaluate the final model on the holdout set\n",
    "    final_model = LogisticRegressionModel(input_dim)\n",
    "    train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "    holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Train the final model with the best weight decay found\n",
    "    holdout_f1, _, holdout_acc, holdout_pred, holdout_all_labels = train_and_evaluate_model(train_loader, holdout_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=best_weight_decay)\n",
    "    print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}')\n",
    "    print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%')\n",
    "    torch.save(final_model.state_dict(), 'path_to_my_model.pth')\n",
    "    models_log.append(final_model)\n",
    "    holdout_log.append(holdout_loader)\n",
    "    true_log.append(holdout_all_labels)\n",
    "    preds_log.append(holdout_pred)\n",
    "\n",
    "    # Evaluate precision and recall on holdout set\n",
    "    final_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in holdout_loader:\n",
    "            outputs = final_model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    holdout_precision = precision_score(all_labels, all_preds)\n",
    "    holdout_recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%')\n",
    "    print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open('output.txt', 'a') as file:\n",
    "        print(f\"Phase {i + 1}\", file=file)\n",
    "        print(f'Best Weight Decay: {best_weight_decay}', file=file)\n",
    "        print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}', file=file)\n",
    "        print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%', file=file)\n",
    "        print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%', file=file)\n",
    "        print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%', file=file)\n",
    "        print('\\n', file=file)\n",
    "        print('Output written to output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcnemar_contingency(true_labels, pred_A, pred_B):\n",
    "    assert len(true_labels) == len(pred_A) == len(pred_B), \"Inputs must have the same length\"\n",
    "    \n",
    "    n00 = 0  # Number of items misclassified by both A and B\n",
    "    n01 = 0  # Number of items misclassified by A but not by B\n",
    "    n10 = 0  # Number of items misclassified by B but not by A\n",
    "    n11 = 0  # Number of items classified correctly by both A and B\n",
    "    \n",
    "    for true_label, pred_a, pred_b in zip(true_labels, pred_A, pred_B):\n",
    "        if pred_a != true_label and pred_b != true_label:\n",
    "            n00 += 1\n",
    "        elif pred_a != true_label and pred_b == true_label:\n",
    "            n01 += 1\n",
    "        elif pred_a == true_label and pred_b != true_label:\n",
    "            n10 += 1\n",
    "        elif pred_a == true_label and pred_b == true_label:\n",
    "            n11 += 1\n",
    "    \n",
    "    return n00, n01, n10, n11\n",
    "\n",
    "n00, n01, n10, n11 = compute_mcnemar_contingency(true_log[0], preds_log[0], preds_log[1])\n",
    "\n",
    "print(f\"n00: {n00}\")\n",
    "print(f\"n01: {n01}\")\n",
    "print(f\"n10: {n10}\")\n",
    "print(f\"n11: {n11}\")\n",
    "\n",
    "cont_table_pred_set = [[n00, n01], [n10, n11]]\n",
    "print(\"Contingency Table for Predictions:\")\n",
    "print(cont_table_pred_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "threshold = 3.841 \n",
    "significance_value = 0.05\n",
    "\n",
    "# McNemar's Test with the continuity correction\n",
    "test = mcnemar(cont_table_pred_set, exact=False, correction=True)\n",
    "\n",
    "if test.pvalue < significance_value:\n",
    "  print(\"Reject Null hypotesis\")\n",
    "else:\n",
    "  print(\"Fail to reject Null hypotesis\")\n",
    "\n",
    "#or equivalently\n",
    "if test.statistic > threshold:\n",
    "  print(\"Reject Null hypotesis\")\n",
    "else:\n",
    "  print(\"Fail to reject Null hypotesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def evaluate_roc_auc(models, loaders):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i, (model, loader) in enumerate(zip(models, loaders)):\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                outputs = model(images)\n",
    "                y_true.extend(labels.numpy())\n",
    "                y_scores.extend(outputs.numpy())\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        auc_score = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f'Model {i} ROC curve (area = {auc_score:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming model0 and model1 are your models and holdout_loader0 and holdout_loader1 are the corresponding loaders\n",
    "evaluate_roc_auc(models_log, holdout_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
