{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "true_features = []\n",
    "with open('Feature_ranking', 'r', encoding=\"utf-8\") as file:\n",
    "    features = file.read()\n",
    "    for line in features.split('\\n'):\n",
    "        if line.endswith('True'):\n",
    "            true_features.append(re.findall(r\"'([^']*)'\", line)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data1 = pd.read_csv('x_matricer/x_matrix_pre_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns=['IDno','dead30','dead90','dead365'])\n",
    "data2 = pd.read_csv('x_matricer/x_matrix_phase_1_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data3 = pd.read_csv('x_matricer/x_matrix_phase_2_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data4 = pd.read_csv('x_matricer/x_matrix_phase_3_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data5 = pd.read_csv('x_matricer/x_matrix_phase_4_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data6 = pd.read_csv('x_matricer/x_matrix_phase_5_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "y_pred = pd.read_csv('y_pred_4729.csv').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "\n",
    "base = data1\n",
    "phase1 = pd.concat([data1, data2], axis = 1)\n",
    "phase2 = pd.concat([data1, data2, data3], axis = 1)\n",
    "phase3 = pd.concat([data1, data2, data3, data4], axis = 1)\n",
    "phase4 = pd.concat([data1, data2, data3, data4, data5], axis = 1)\n",
    "phase5 = pd.concat([data1, data2, data3, data4, data5, data6], axis = 1)\n",
    "\n",
    "data_list = [base, phase1, phase2, phase3, phase4, phase5]\n",
    "preds_log = []\n",
    "models_log = []\n",
    "holdout_log = []\n",
    "true_log = []\n",
    "for i in range(len(data_list)):\n",
    "    data = data_list[i]\n",
    "    # data = data[data.columns.intersection(true_features)]\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Convert to numpy array and generate synthetic labels for demonstration\n",
    "    X = data.to_numpy()\n",
    "    y = y_pred.to_numpy()\n",
    "    # Standardize the data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).flatten()\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Split data into holdout set and remaining set\n",
    "    train_indices, holdout_indices = train_test_split(np.arange(len(dataset)), test_size=500, random_state=42, stratify=y_tensor.numpy())\n",
    "\n",
    "    holdout_set = Subset(dataset, holdout_indices)\n",
    "    remaining_set = Subset(dataset, train_indices)\n",
    "\n",
    "    y_1_percent = np.count_nonzero(y_tensor) / len(y_tensor)\n",
    "    y_0_percent = 1 - y_1_percent\n",
    "    print(f\"Class 1 percentage: {y_1_percent * 100:.2f}%\")\n",
    "    print(f\"Class 0 percentage: {y_0_percent * 100:.2f}%\")\n",
    "    # Define class weights for BCEWithLogitsLoss\n",
    "    class_weights = torch.tensor([1/y_0_percent, 1/y_1_percent])\n",
    "\n",
    "    # Define the logistic regression model with L2 regularization\n",
    "    class LogisticRegressionModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(LogisticRegressionModel, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = torch.sigmoid(self.linear(x))\n",
    "            return out\n",
    "\n",
    "    input_dim = data.shape[1]\n",
    "\n",
    "    # Function to get target tensor from Subset\n",
    "    def get_targets(subset):\n",
    "        return subset.dataset.tensors[1][subset.indices]\n",
    "\n",
    "    # Get target tensors for holdout and remaining sets\n",
    "    holdout_targets = get_targets(holdout_set)\n",
    "    remaining_targets = get_targets(remaining_set)\n",
    "\n",
    "    # Print class distributions\n",
    "    print(f\"Class distribution in holdout set: {np.unique(holdout_targets.numpy(), return_counts=True)}\")\n",
    "    print(f\"Class distribution in remaining set: {np.unique(remaining_targets.numpy(), return_counts=True)}\")\n",
    "\n",
    "    weight_decay_values = np.logspace(-5, 1, 7)  # for example, try values from 1e-5 to 10\n",
    "\n",
    "    # Function to train and evaluate the model with L2 regularization\n",
    "    def train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "        model = LogisticRegressionModel(input_dim)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                predicted = (outputs > 0.5).float().squeeze()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        accuracy = correct / total\n",
    "        return f1, model, accuracy, all_preds, all_labels\n",
    "\n",
    "    outer_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # Function to perform grid search over weight_decay values\n",
    "    def grid_search(train_set, outer_kf, weight_decays, input_dim, num_epochs=10, lr=0.01):\n",
    "        best_weight_decay = None\n",
    "        best_outer_f1 = 0\n",
    "        y_remaining = y_tensor[train_indices].numpy()\n",
    "\n",
    "        for weight_decay in weight_decays:\n",
    "            outer_f1_scores = []\n",
    "            \n",
    "            for outer_train_index, outer_test_index in outer_kf.split(np.zeros(len(train_set)), y_remaining):\n",
    "                \n",
    "                outer_train_subset = Subset(train_set, outer_train_index)\n",
    "                outer_test_subset = Subset(train_set, outer_test_index)\n",
    "                \n",
    "                # Inner 5-Fold Cross-Validation\n",
    "                inner_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                inner_f1_scores = []\n",
    "                \n",
    "                for inner_train_index, inner_val_index in inner_kf.split(np.zeros(len(outer_train_index)), y_remaining[outer_train_index]):\n",
    "                    \n",
    "                    inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "                    inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "                    \n",
    "                    inner_train_subset = Subset(train_set, inner_train_indices)\n",
    "                    inner_val_subset = Subset(train_set, inner_val_indices)\n",
    "                    \n",
    "                    train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "                    val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "                    \n",
    "                    f1, _, _, _, _ = train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs, lr, weight_decay)\n",
    "                    inner_f1_scores.append(f1)\n",
    "                            \n",
    "                # Train on outer train subset and evaluate on outer test subset\n",
    "                train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "                test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "                \n",
    "                outer_f1, _, _, _, _ = train_and_evaluate_model(train_loader, test_loader, input_dim, num_epochs, lr, weight_decay)\n",
    "                outer_f1_scores.append(outer_f1)\n",
    "            \n",
    "            mean_outer_f1 = np.mean(outer_f1_scores)\n",
    "            print(f'Weight Decay: {weight_decay}, Mean Outer F1 Score: {mean_outer_f1:.4f}')\n",
    "            \n",
    "            if mean_outer_f1 > best_outer_f1:\n",
    "                best_outer_f1 = mean_outer_f1\n",
    "                best_weight_decay = weight_decay\n",
    "        \n",
    "        return best_weight_decay\n",
    "\n",
    "\n",
    "    # Perform grid search over weight_decay values\n",
    "\n",
    "    best_weight_decay = grid_search(remaining_set, outer_kf, weight_decay_values, input_dim, num_epochs=10, lr=0.01)\n",
    "\n",
    "    print(f'Best Weight Decay: {best_weight_decay}')\n",
    "\n",
    "    # Evaluate the final model on the holdout set\n",
    "    final_model = LogisticRegressionModel(input_dim)\n",
    "    train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "    holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Train the final model with the best weight decay found\n",
    "    holdout_f1, _, holdout_acc, holdout_pred, holdout_all_labels = train_and_evaluate_model(train_loader, holdout_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=best_weight_decay)\n",
    "    print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}')\n",
    "    print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%')\n",
    "    torch.save(final_model.state_dict(), f\"Variables_log_reg/phase {i}\")\n",
    "    models_log.append(final_model)\n",
    "    holdout_log.append(holdout_loader)\n",
    "    true_log.append(holdout_all_labels)\n",
    "    preds_log.append(holdout_pred)\n",
    "\n",
    "    # Evaluate precision and recall on holdout set\n",
    "    final_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in holdout_loader:\n",
    "            outputs = final_model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    holdout_precision = precision_score(all_labels, all_preds)\n",
    "    holdout_recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%')\n",
    "    print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open('Variables_log_reg/results.txt', 'a') as file:\n",
    "        print(f\"Phase {i}\", file=file)\n",
    "        print(f'Best Weight Decay: {best_weight_decay}', file=file)\n",
    "        print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}', file=file)\n",
    "        print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%', file=file)\n",
    "        print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%', file=file)\n",
    "        print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%', file=file)\n",
    "        print('\\n', file=file)\n",
    "        print('Output written to output.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = 'Variables_log_reg/preds_log.pickle'\n",
    "\n",
    "# Open the file in binary mode\n",
    "with open(file_path, 'wb') as file:\n",
    "    # Serialize and write the variable to the file\n",
    "    pickle.dump(preds_log, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('x_matricer/x_matrix_pre_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns=['IDno','dead30','dead90','dead365'])\n",
    "data2 = pd.read_csv('x_matricer/x_matrix_phase_1_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data3 = pd.read_csv('x_matricer/x_matrix_phase_2_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data4 = pd.read_csv('x_matricer/x_matrix_phase_3_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data5 = pd.read_csv('x_matricer/x_matrix_phase_4_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "data6 = pd.read_csv('x_matricer/x_matrix_phase_5_4729.csv', compression = 'gzip').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "y_pred = pd.read_csv('y_pred_4729.csv').sort_values(by='IDno',ignore_index=True).drop(columns='IDno')\n",
    "\n",
    "base = data1\n",
    "phase1 = pd.concat([data1, data2], axis = 1)\n",
    "phase2 = pd.concat([data1, data2, data3], axis = 1)\n",
    "phase3 = pd.concat([data1, data2, data3, data4], axis = 1)\n",
    "phase4 = pd.concat([data1, data2, data3, data4, data5], axis = 1)\n",
    "phase5 = pd.concat([data1, data2, data3, data4, data5, data6], axis = 1)\n",
    "\n",
    "data_list = [base, phase1, phase2, phase3, phase4, phase5]\n",
    "preds_log = []\n",
    "models_log = []\n",
    "holdout_log = []\n",
    "true_log = []\n",
    "for i in range(len(data_list)):\n",
    "    data = data_list[i]\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Convert to numpy array and generate synthetic labels for demonstration\n",
    "    X = data.to_numpy()\n",
    "    y = y_pred.to_numpy()\n",
    "    # Standardize the data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).flatten()\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Split data into holdout set and remaining set\n",
    "    train_indices, holdout_indices = train_test_split(np.arange(len(dataset)), test_size=500, random_state=42, stratify=y_tensor.numpy())\n",
    "\n",
    "    holdout_set = Subset(dataset, holdout_indices)\n",
    "    remaining_set = Subset(dataset, train_indices)\n",
    "\n",
    "    y_1_percent = np.count_nonzero(y_tensor) / len(y_tensor)\n",
    "    y_0_percent = 1 - y_1_percent\n",
    "    print(f\"Class 1 percentage: {y_1_percent * 100:.2f}%\")\n",
    "    print(f\"Class 0 percentage: {y_0_percent * 100:.2f}%\")\n",
    "    # Define class weights for BCEWithLogitsLoss\n",
    "    class_weights = torch.tensor([1/y_0_percent, 1/y_1_percent])\n",
    "\n",
    "    # Define the logistic regression model with L2 regularization\n",
    "    class LogisticRegressionModel(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(LogisticRegressionModel, self).__init__()\n",
    "            self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out = torch.sigmoid(self.linear(x))\n",
    "            return out\n",
    "\n",
    "    input_dim = data.shape[1]\n",
    "\n",
    "    # Function to get target tensor from Subset\n",
    "    def get_targets(subset):\n",
    "        return subset.dataset.tensors[1][subset.indices]\n",
    "\n",
    "    # Get target tensors for holdout and remaining sets\n",
    "    holdout_targets = get_targets(holdout_set)\n",
    "    remaining_targets = get_targets(remaining_set)    \n",
    "\n",
    "\n",
    "# Evaluate the final model on the holdout set\n",
    "    model = #mo\n",
    "    train_loader = DataLoader(remaining_set, batch_size=64, shuffle=True)\n",
    "    holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Train the final model with the best weight decay found\n",
    "    holdout_f1, _, holdout_acc, holdout_pred, holdout_all_labels = train_and_evaluate_model(train_loader, holdout_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=best_weight_decay)\n",
    "    print(f'Final Model F1 Score on Holdout Set: {holdout_f1:.4f}')\n",
    "    print(f'Holdout Set Accuracy: {holdout_acc * 100:.2f}%')\n",
    "    models_log.append(final_model)\n",
    "    holdout_log.append(holdout_loader)\n",
    "    true_log.append(holdout_all_labels)\n",
    "    preds_log.append(holdout_pred)\n",
    "\n",
    "        # Evaluate precision and recall on holdout set\n",
    "    final_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in holdout_loader:\n",
    "            outputs = final_model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    holdout_precision = precision_score(all_labels, all_preds)\n",
    "    holdout_recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    print(f'Holdout Set Precision: {holdout_precision * 100:.2f}%')\n",
    "    print(f'Holdout Set Recall: {holdout_recall * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "true_features = []\n",
    "with open('Feature_ranking', 'r') as file:\n",
    "    features = file.read()\n",
    "    for line in features.split('\\n'):\n",
    "        if line.endswith('True'):\n",
    "            true_features.append(re.findall(r\"'([^']*)'\", line)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "def compute_mcnemar_contingency(true_labels, pred_A, pred_B):\n",
    "    assert len(true_labels) == len(pred_A) == len(pred_B), \"Inputs must have the same length\"\n",
    "    \n",
    "    n00 = 0  # Number of items misclassified by both A and B\n",
    "    n01 = 0  # Number of items misclassified by A but not by B\n",
    "    n10 = 0  # Number of items misclassified by B but not by A\n",
    "    n11 = 0  # Number of items classified correctly by both A and B\n",
    "    \n",
    "    for true_label, pred_a, pred_b in zip(true_labels, pred_A, pred_B):\n",
    "        if pred_a != true_label and pred_b != true_label:\n",
    "            n00 += 1\n",
    "        elif pred_a != true_label and pred_b == true_label:\n",
    "            n01 += 1\n",
    "        elif pred_a == true_label and pred_b != true_label:\n",
    "            n10 += 1\n",
    "        elif pred_a == true_label and pred_b == true_label:\n",
    "            n11 += 1\n",
    "    \n",
    "    return n00, n01, n10, n11\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from decimal import Decimal\n",
    "\n",
    "names = ['pre', 'phase1', 'phase2', 'phase3', 'phase4', 'phase5']\n",
    "\n",
    "# Initialize a dictionary to store p-values\n",
    "p_value_table = {}\n",
    "\n",
    "# Generate all combinations of pairs of indices\n",
    "comparison_pairs = list(combinations(range(len(names)), 2))\n",
    "\n",
    "for i, j in comparison_pairs:\n",
    "    n00, n01, n10, n11 = compute_mcnemar_contingency(true_log[0], preds_log[i], preds_log[j])\n",
    "    cont_table_pred_set = [[n00, n01], [n10, n11]]\n",
    "    \n",
    "    # Perform McNemar's test\n",
    "    result = mcnemar(cont_table_pred_set)\n",
    "    \n",
    "    # Store p-value in the table for both (i, j) and (j, i)\n",
    "    p_value_table.setdefault(names[i], {})[names[j]] = '%.2E' % Decimal(result.pvalue)\n",
    "    p_value_table.setdefault(names[j], {})[names[i]] = '%.2E' % Decimal(result.pvalue)\n",
    "\n",
    "# Convert the nested dictionary to a DataFrame\n",
    "p_value_df = pd.DataFrame.from_dict(p_value_table, orient='index')\n",
    "\n",
    "# Ensure rows and columns follow the order of names\n",
    "p_value_df = p_value_df.reindex(index=names, columns=names)\n",
    "\n",
    "print(\"P-Value Table:\")\n",
    "print(p_value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def evaluate_roc_auc(models, loaders):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i, (model, loader) in enumerate(zip(models, loaders)):\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                outputs = model(images)\n",
    "                y_true.extend(labels.numpy())\n",
    "                y_scores.extend(outputs.numpy())\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "        auc_score = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f'Model {i} ROC curve (area = {auc_score:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming model0 and model1 are your models and holdout_loader0 and holdout_loader1 are the corresponding loaders\n",
    "evaluate_roc_auc(models_log, holdout_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your data and set up your variables as before...\n",
    "\n",
    "data1 = pd.read_csv('x_matricer/x_matrix_pre_4729.csv', compression='gzip').sort_values(by='IDno', ignore_index=True).drop(columns=['IDno','dead30','dead90','dead365'])\n",
    "data2 = pd.read_csv('x_matricer/x_matrix_phase_1_4729.csv', compression='gzip').sort_values(by='IDno', ignore_index=True).drop(columns='IDno')\n",
    "data3 = pd.read_csv('x_matricer/x_matrix_phase_2_4729.csv', compression='gzip').sort_values(by='IDno', ignore_index=True).drop(columns='IDno')\n",
    "data4 = pd.read_csv('x_matricer/x_matrix_phase_3_4729.csv', compression='gzip').sort_values(by='IDno', ignore_index=True).drop(columns='IDno')\n",
    "data5 = pd.read_csv('x_matricer/x_matrix_phase_4_4729.csv', compression='gzip').sort_values(by='IDno', ignore_index=True).drop(columns='IDno')\n",
    "data6 = pd.read_csv('x_matricer/x_matrix_phase_5_4729.csv', compression='gzip').sort_values(by='IDno', ignore_index=True).drop(columns='IDno')\n",
    "y_pred = pd.read_csv('y_pred_4729.csv').sort_values(by='IDno', ignore_index=True).drop(columns='IDno')\n",
    "\n",
    "base = data1\n",
    "phase1 = pd.concat([data1, data2], axis=1)\n",
    "phase2 = pd.concat([data1, data2, data3], axis=1)\n",
    "phase3 = pd.concat([data1, data2, data3, data4], axis=1)\n",
    "phase4 = pd.concat([data1, data2, data3, data4, data5], axis=1)\n",
    "phase5 = pd.concat([data1, data2, data3, data4, data5, data6], axis=1)\n",
    "\n",
    "data_list = [base, phase1, phase2, phase3, phase4, phase5]\n",
    "# preds_log = []\n",
    "# models_log = []\n",
    "# holdout_log = []\n",
    "# true_log = []\n",
    "\n",
    "data = phase5\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert to numpy array and generate synthetic labels for demonstration\n",
    "X = data.to_numpy()\n",
    "y = y_pred.to_numpy().flatten()\n",
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Perform Recursive Feature Elimination with Cross-Validation (RFECV)\n",
    "estimator = LogisticRegression(random_state=42)\n",
    "rfecv = RFECV(estimator=estimator, step=5, cv=3, scoring='f1')\n",
    "rfecv.fit(X, y)\n",
    "    \n",
    "# Get the selected features\n",
    "selected_features = rfecv.support_\n",
    "X_selected = X[:, selected_features]\n",
    "    \n",
    "# Print feature rankings\n",
    "print(f\"Phase 5 Feature Rankings:\")\n",
    "for rank, (feature, support) in enumerate(zip(data.columns, rfecv.support_), start=1):\n",
    "    print(f\"Rank {rank}: Feature '{feature}', Support: {support}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
