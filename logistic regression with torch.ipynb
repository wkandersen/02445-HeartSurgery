{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Fold Accuracy: 46.69%\n",
      "Inner Fold Accuracy: 51.24%\n",
      "Inner Fold Accuracy: 45.87%\n",
      "Inner Fold Accuracy: 44.21%\n",
      "Inner Fold Accuracy: 46.69%\n",
      "Inner Fold Accuracy: 47.93%\n",
      "Inner Fold Accuracy: 54.36%\n",
      "Inner Fold Accuracy: 43.57%\n",
      "Inner Fold Accuracy: 47.72%\n",
      "Inner Fold Accuracy: 49.79%\n",
      "Outer Fold Mean Inner Accuracy: 47.81%\n",
      "Outer Fold Accuracy: 50.93%\n",
      "Inner Fold Accuracy: 42.15%\n",
      "Inner Fold Accuracy: 46.69%\n",
      "Inner Fold Accuracy: 55.37%\n",
      "Inner Fold Accuracy: 52.07%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 50.83%\n",
      "Inner Fold Accuracy: 50.21%\n",
      "Inner Fold Accuracy: 54.36%\n",
      "Inner Fold Accuracy: 48.96%\n",
      "Inner Fold Accuracy: 50.21%\n",
      "Outer Fold Mean Inner Accuracy: 49.84%\n",
      "Outer Fold Accuracy: 49.07%\n",
      "Inner Fold Accuracy: 48.76%\n",
      "Inner Fold Accuracy: 51.24%\n",
      "Inner Fold Accuracy: 43.39%\n",
      "Inner Fold Accuracy: 54.13%\n",
      "Inner Fold Accuracy: 47.11%\n",
      "Inner Fold Accuracy: 48.76%\n",
      "Inner Fold Accuracy: 48.96%\n",
      "Inner Fold Accuracy: 49.79%\n",
      "Inner Fold Accuracy: 47.72%\n",
      "Inner Fold Accuracy: 47.72%\n",
      "Outer Fold Mean Inner Accuracy: 48.76%\n",
      "Outer Fold Accuracy: 45.72%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 48.35%\n",
      "Inner Fold Accuracy: 50.00%\n",
      "Inner Fold Accuracy: 46.69%\n",
      "Inner Fold Accuracy: 49.59%\n",
      "Inner Fold Accuracy: 54.96%\n",
      "Inner Fold Accuracy: 49.79%\n",
      "Inner Fold Accuracy: 51.04%\n",
      "Inner Fold Accuracy: 47.72%\n",
      "Inner Fold Accuracy: 51.45%\n",
      "Outer Fold Mean Inner Accuracy: 49.71%\n",
      "Outer Fold Accuracy: 46.10%\n",
      "Inner Fold Accuracy: 50.41%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 52.07%\n",
      "Inner Fold Accuracy: 47.11%\n",
      "Inner Fold Accuracy: 51.24%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 51.87%\n",
      "Inner Fold Accuracy: 49.79%\n",
      "Inner Fold Accuracy: 50.62%\n",
      "Inner Fold Accuracy: 57.26%\n",
      "Outer Fold Mean Inner Accuracy: 50.54%\n",
      "Outer Fold Accuracy: 46.84%\n",
      "Inner Fold Accuracy: 53.31%\n",
      "Inner Fold Accuracy: 45.87%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 51.24%\n",
      "Inner Fold Accuracy: 50.83%\n",
      "Inner Fold Accuracy: 45.04%\n",
      "Inner Fold Accuracy: 47.11%\n",
      "Inner Fold Accuracy: 51.87%\n",
      "Inner Fold Accuracy: 46.47%\n",
      "Inner Fold Accuracy: 51.04%\n",
      "Outer Fold Mean Inner Accuracy: 49.03%\n",
      "Outer Fold Accuracy: 45.90%\n",
      "Inner Fold Accuracy: 52.07%\n",
      "Inner Fold Accuracy: 47.93%\n",
      "Inner Fold Accuracy: 50.41%\n",
      "Inner Fold Accuracy: 53.72%\n",
      "Inner Fold Accuracy: 51.65%\n",
      "Inner Fold Accuracy: 46.28%\n",
      "Inner Fold Accuracy: 45.87%\n",
      "Inner Fold Accuracy: 50.21%\n",
      "Inner Fold Accuracy: 51.04%\n",
      "Inner Fold Accuracy: 50.21%\n",
      "Outer Fold Mean Inner Accuracy: 49.94%\n",
      "Outer Fold Accuracy: 48.13%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 45.87%\n",
      "Inner Fold Accuracy: 51.24%\n",
      "Inner Fold Accuracy: 50.00%\n",
      "Inner Fold Accuracy: 50.00%\n",
      "Inner Fold Accuracy: 45.87%\n",
      "Inner Fold Accuracy: 51.24%\n",
      "Inner Fold Accuracy: 54.77%\n",
      "Inner Fold Accuracy: 49.79%\n",
      "Inner Fold Accuracy: 48.55%\n",
      "Outer Fold Mean Inner Accuracy: 49.48%\n",
      "Outer Fold Accuracy: 48.13%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 50.41%\n",
      "Inner Fold Accuracy: 47.93%\n",
      "Inner Fold Accuracy: 47.52%\n",
      "Inner Fold Accuracy: 55.37%\n",
      "Inner Fold Accuracy: 48.76%\n",
      "Inner Fold Accuracy: 50.83%\n",
      "Inner Fold Accuracy: 45.23%\n",
      "Inner Fold Accuracy: 45.23%\n",
      "Inner Fold Accuracy: 51.45%\n",
      "Outer Fold Mean Inner Accuracy: 49.03%\n",
      "Outer Fold Accuracy: 45.90%\n",
      "Inner Fold Accuracy: 45.87%\n",
      "Inner Fold Accuracy: 48.35%\n",
      "Inner Fold Accuracy: 50.83%\n",
      "Inner Fold Accuracy: 50.41%\n",
      "Inner Fold Accuracy: 47.93%\n",
      "Inner Fold Accuracy: 54.13%\n",
      "Inner Fold Accuracy: 45.04%\n",
      "Inner Fold Accuracy: 51.04%\n",
      "Inner Fold Accuracy: 42.32%\n",
      "Inner Fold Accuracy: 49.79%\n",
      "Outer Fold Mean Inner Accuracy: 48.57%\n",
      "Outer Fold Accuracy: 53.36%\n",
      "Mean Outer Accuracy: 48.01%\n",
      "Best model parameters: OrderedDict({'linear.weight': tensor([[ 6.5237e-03,  4.9266e-02, -2.9048e-02,  5.0663e-03,  5.5003e-02,\n",
      "         -3.8324e-03, -1.0869e-02, -3.1783e-02, -6.5092e-03,  3.6945e-02,\n",
      "          1.8790e-02,  5.5966e-02,  1.5658e-02, -1.5947e-02, -2.2847e-02,\n",
      "         -1.0555e-03,  7.0964e-03, -7.4299e-03,  2.6618e-02, -1.2105e-02,\n",
      "          2.5857e-02,  2.7403e-02, -5.2863e-03,  9.6817e-04,  1.0888e-02,\n",
      "         -3.4828e-03, -2.3469e-02,  1.0921e-02,  6.3845e-03, -1.4909e-02,\n",
      "          6.7774e-03, -6.7867e-03, -1.2551e-02,  7.2073e-04, -1.2731e-02,\n",
      "         -2.5396e-03, -4.7636e-02,  1.3312e-02, -1.3109e-02,  3.7535e-03,\n",
      "         -4.4685e-02, -3.5114e-03, -4.8435e-02, -3.0130e-02, -1.4976e-02,\n",
      "         -3.2086e-02, -2.6577e-02,  1.1039e-03,  2.9236e-02, -6.7708e-03,\n",
      "          3.8450e-02,  8.8872e-03,  5.3855e-02,  2.0096e-02,  1.7427e-03,\n",
      "         -2.0429e-02,  4.3861e-02,  1.1904e-03,  1.6782e-04, -3.7651e-02,\n",
      "          1.9962e-03, -2.2694e-02, -9.0347e-03,  3.4103e-02,  7.3538e-03,\n",
      "         -2.3786e-03, -2.3800e-03, -3.0603e-02, -1.9004e-03,  8.6300e-03,\n",
      "         -1.2274e-02, -2.7819e-02, -6.1817e-02,  2.1860e-02, -1.0079e-02,\n",
      "          3.0062e-02, -3.4251e-03, -3.6733e-03,  8.0555e-03, -1.7065e-02,\n",
      "         -1.1617e-02, -9.0455e-03, -1.0080e-02, -2.1421e-02, -4.4548e-02,\n",
      "         -8.1316e-05,  7.4040e-02,  3.9766e-04, -3.4468e-04, -3.8030e-02,\n",
      "         -2.3208e-02, -7.4742e-03,  2.5536e-02, -5.6724e-03, -2.2453e-03,\n",
      "         -3.9216e-02, -9.2966e-05, -3.5610e-02, -1.3579e-02, -4.2384e-02,\n",
      "         -3.9659e-03, -9.7871e-03, -2.8930e-02, -9.1273e-03, -4.5083e-03,\n",
      "         -1.0906e-02,  7.9745e-03,  2.1611e-02, -8.5090e-03, -7.0225e-02,\n",
      "         -1.2105e-02,  1.4108e-02,  1.7502e-02, -1.6540e-02, -1.0649e-02,\n",
      "          5.5207e-02, -2.4761e-02,  6.3032e-02, -4.8218e-02,  3.1671e-02,\n",
      "         -1.2098e-02,  1.5412e-02,  4.6731e-02,  1.4735e-02, -2.6966e-03,\n",
      "         -1.3715e-02,  6.8820e-03, -7.6374e-02, -7.0682e-03,  2.0925e-02,\n",
      "          1.6795e-02,  1.9103e-02,  2.3654e-02,  6.9201e-03,  4.8934e-02,\n",
      "          5.5074e-02,  3.6569e-02,  1.2351e-02, -4.8228e-02,  3.3739e-02,\n",
      "          7.2470e-03,  6.4373e-03, -1.9900e-02,  3.8135e-03,  2.4181e-02,\n",
      "          4.7521e-02,  9.3905e-03,  4.3716e-03, -3.5032e-02,  1.4605e-02,\n",
      "          2.9166e-02, -1.5299e-02, -1.0868e-02, -3.9482e-02, -1.4085e-02,\n",
      "         -2.9153e-02, -4.9839e-02,  2.6399e-02,  6.5657e-03, -3.6784e-03,\n",
      "          2.5700e-02,  4.1747e-02,  5.9954e-03, -6.2770e-02, -4.4340e-02,\n",
      "         -2.8679e-02, -3.2883e-02, -2.5257e-02,  2.4565e-02, -2.4974e-02,\n",
      "         -1.8986e-02, -5.9312e-02, -3.0300e-02, -1.4034e-02, -3.0822e-03,\n",
      "          3.1287e-02, -1.2524e-02, -9.3291e-03, -4.8574e-03, -2.9292e-02,\n",
      "          7.0016e-03, -2.4473e-03,  2.6280e-02,  6.1957e-02, -6.9410e-03,\n",
      "         -2.2755e-02,  3.7430e-02,  4.7421e-02,  1.5340e-02, -2.2058e-02,\n",
      "         -2.7555e-03, -3.3357e-02, -4.2389e-02, -4.9440e-02, -2.5980e-02,\n",
      "          1.2100e-02,  1.5855e-02, -8.2275e-03,  2.4629e-03, -1.6543e-02,\n",
      "         -5.6109e-03,  1.1615e-02, -1.1386e-02, -3.4794e-02, -7.7422e-03,\n",
      "          8.7900e-03,  1.2817e-02,  9.5417e-03, -1.9755e-02, -2.3802e-02,\n",
      "         -1.2792e-02,  1.1319e-02,  5.8725e-02,  4.2406e-03,  3.6500e-02,\n",
      "         -1.9477e-02, -5.5137e-03,  3.5080e-02, -6.7495e-03,  4.0436e-03,\n",
      "          2.6236e-02, -4.2583e-02, -4.2907e-03, -1.0227e-02, -7.8738e-03,\n",
      "          2.7922e-03, -2.9523e-02, -4.6578e-02,  4.3046e-02, -3.0122e-02,\n",
      "          8.2912e-03, -2.4685e-03, -1.7116e-02,  1.6077e-02, -4.4662e-03,\n",
      "          5.8566e-03,  2.6874e-02,  6.7267e-02,  5.6742e-02,  1.4391e-02,\n",
      "          4.0708e-03,  3.4654e-04, -8.2802e-03,  3.5403e-02, -1.0106e-02,\n",
      "         -5.7857e-03, -3.5390e-02,  2.3859e-02, -2.0048e-02, -2.1672e-02,\n",
      "          4.3082e-02, -2.6515e-02,  3.3408e-03, -4.6937e-03,  5.7463e-03,\n",
      "         -2.9759e-02, -1.0703e-02,  2.5558e-02, -2.3998e-02, -4.5581e-02,\n",
      "          5.3262e-02,  3.0848e-02, -3.3426e-02, -3.2346e-02, -1.4608e-02,\n",
      "         -2.8301e-02,  2.8446e-02,  2.9190e-02,  2.7427e-02,  2.0864e-02,\n",
      "          6.1504e-02,  2.6571e-03, -6.0847e-02, -3.3790e-03, -1.6244e-03,\n",
      "         -3.2818e-02, -2.7486e-02,  6.7212e-03, -9.2111e-03, -4.1856e-02,\n",
      "          7.1094e-03, -5.5451e-03, -1.5663e-02,  4.2020e-02,  1.0579e-02,\n",
      "         -2.9709e-03,  3.2640e-03, -2.5098e-02,  4.7444e-02,  6.9500e-03,\n",
      "          2.9590e-02, -4.3615e-02, -4.6502e-02,  3.9962e-02,  1.3094e-02,\n",
      "          2.2148e-02, -2.5136e-02, -1.0838e-02,  4.2408e-03, -3.1120e-02,\n",
      "          4.8110e-02, -7.1812e-04, -2.1607e-02, -1.1335e-02,  1.5263e-02,\n",
      "         -3.8087e-03, -1.0120e-02,  1.3037e-02,  5.2453e-02,  1.5365e-02,\n",
      "         -2.2384e-02,  1.7920e-02,  1.9477e-02, -5.2132e-04,  2.5250e-02,\n",
      "          4.3027e-02,  9.8655e-03,  3.5516e-02, -2.1121e-02, -3.8674e-03,\n",
      "         -4.7764e-02, -2.3052e-02, -1.4408e-02, -2.8576e-02, -9.6522e-03,\n",
      "          1.4044e-02, -1.1257e-02,  7.7860e-03, -4.6536e-02, -1.2224e-02,\n",
      "         -1.6110e-02, -3.1053e-02, -1.7004e-02, -2.5517e-02,  7.1429e-03,\n",
      "         -4.7694e-02,  2.7144e-02,  4.6490e-02,  2.7983e-02, -7.5133e-03,\n",
      "         -3.5192e-02,  2.2767e-02, -2.6788e-02, -5.8688e-02, -1.7971e-02,\n",
      "          2.9683e-02, -3.1518e-02,  1.3567e-02, -2.5561e-02, -1.2387e-02,\n",
      "         -5.0699e-04, -2.6844e-02, -1.8990e-02,  1.8767e-02,  6.4209e-02,\n",
      "          3.2567e-02,  1.2557e-02, -6.0323e-02,  1.6293e-02,  1.7167e-02,\n",
      "          5.3693e-02,  3.4666e-03,  3.2906e-03,  4.7363e-02, -1.6746e-02,\n",
      "          1.2996e-02,  1.5755e-03,  5.7959e-03,  5.6076e-03, -2.6216e-02,\n",
      "          1.7190e-02, -2.5686e-02, -3.9554e-02, -4.0771e-02,  8.2674e-03,\n",
      "         -3.3946e-02,  5.2373e-02,  2.6347e-02,  1.2394e-02,  5.6291e-02,\n",
      "          2.5993e-02, -7.4222e-03, -1.9090e-02,  3.3734e-02, -2.3195e-02,\n",
      "          3.0568e-02,  2.7326e-02,  1.5294e-02, -1.6597e-02,  4.3824e-02,\n",
      "         -4.6481e-03, -4.6433e-02, -4.3429e-02, -1.2359e-02, -8.2712e-03,\n",
      "         -1.6513e-02, -1.9246e-02, -2.3083e-02, -3.8259e-02,  1.3560e-02,\n",
      "         -9.2458e-03, -1.1893e-02,  7.0731e-05, -3.3154e-02,  3.8933e-03,\n",
      "          2.7564e-02, -2.2885e-02,  5.4144e-04,  3.5566e-02,  1.7969e-02,\n",
      "         -1.9471e-02, -3.3839e-02, -4.2084e-02,  8.5746e-03, -3.7430e-04,\n",
      "         -3.0954e-03,  4.7692e-02,  1.3344e-02, -6.3103e-02, -2.9640e-02,\n",
      "         -2.4706e-02,  6.8959e-03, -1.6259e-02, -1.1007e-02,  3.9387e-02,\n",
      "          5.6463e-03, -2.0932e-02, -2.6119e-02, -2.8510e-02,  1.4452e-03,\n",
      "          1.1237e-04, -4.3073e-03,  1.8403e-02,  4.8024e-02,  2.6141e-02,\n",
      "         -2.7114e-02, -2.5149e-02, -5.3313e-02,  3.8281e-02, -3.6668e-02,\n",
      "         -1.4177e-03, -4.9340e-02,  2.0331e-02,  7.3402e-03, -1.2508e-02,\n",
      "          2.2851e-02, -4.0000e-02,  5.3021e-03, -3.3616e-02,  1.5595e-02,\n",
      "         -4.0654e-02, -2.4331e-02, -9.7874e-03,  2.6681e-02, -3.4223e-03,\n",
      "          2.8055e-03,  1.4268e-02, -3.8444e-02, -4.8423e-02, -3.4522e-02,\n",
      "          8.4345e-03, -4.7798e-03, -4.6147e-02,  7.9081e-03,  2.6584e-02,\n",
      "          8.1946e-04, -2.9803e-02, -1.4740e-02,  3.6145e-02,  9.5038e-03,\n",
      "         -3.4202e-02, -1.2026e-02, -1.2105e-02, -8.5121e-03, -2.9181e-03,\n",
      "          5.0169e-03, -7.7871e-02, -7.0593e-03,  1.7552e-02,  5.1183e-02,\n",
      "         -1.4428e-02, -3.2493e-02,  4.4922e-03, -1.4648e-02,  3.7010e-02,\n",
      "          9.1259e-03,  5.7401e-02,  9.9211e-03,  6.0807e-03,  4.7647e-02,\n",
      "         -1.1482e-02,  1.5001e-02,  2.8258e-03,  1.2114e-02, -2.8291e-02,\n",
      "          7.8187e-03,  2.3766e-02, -2.3980e-03, -4.8858e-02,  9.1533e-04,\n",
      "          4.6433e-02, -6.4131e-03, -2.3059e-03,  3.9083e-02,  2.1075e-02,\n",
      "         -8.2197e-03, -1.2834e-02,  3.0882e-02,  1.6333e-02,  4.6825e-04,\n",
      "          3.7703e-02, -3.2978e-03,  2.0170e-02, -1.2447e-02, -2.8658e-02,\n",
      "         -1.7397e-02, -2.5371e-03, -3.2279e-03,  4.7087e-02, -2.5807e-02,\n",
      "          4.1010e-02, -1.4035e-02,  2.9165e-02, -4.5652e-03, -2.4963e-02,\n",
      "          2.2659e-02,  3.7761e-02, -8.9462e-03,  8.8627e-03,  1.5123e-02,\n",
      "         -1.5928e-02,  3.9981e-02, -3.1356e-02, -6.3016e-03, -3.2008e-02,\n",
      "          2.0942e-03,  2.5709e-04, -3.4274e-02, -2.0472e-02,  4.3737e-02,\n",
      "         -5.9084e-02,  2.0989e-03, -4.8091e-02, -1.2124e-02, -1.3896e-02,\n",
      "          1.9337e-02, -3.9413e-02, -1.2488e-02,  1.2069e-02,  1.3253e-02,\n",
      "         -1.4225e-02, -2.2255e-02,  2.3799e-02, -2.6355e-02, -6.3702e-03,\n",
      "          9.4689e-03, -2.1321e-02, -8.3931e-03,  2.1010e-02,  3.3988e-02,\n",
      "          5.2362e-02, -2.1446e-02,  4.5550e-03, -8.2682e-03,  1.2372e-02,\n",
      "          9.9751e-03,  1.5150e-02,  1.6778e-02, -4.5277e-02,  5.3491e-04,\n",
      "          3.2433e-03,  1.2720e-02,  2.2344e-02, -2.3623e-02, -2.0360e-02]]), 'linear.bias': tensor([0.0127])})\n",
      "Holdout Set Accuracy: 50.40%\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('pseudodata_præoperation.csv')\n",
    "\n",
    "#make pd to np\n",
    "X = data.to_numpy()\n",
    "y = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create a dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split data into holdout set and remaining set\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "holdout_indices = indices[:500]\n",
    "remaining_indices = indices[500:]\n",
    "\n",
    "holdout_set = Subset(dataset, holdout_indices)\n",
    "remaining_set = Subset(dataset, remaining_indices)\n",
    "\n",
    "# Define the logistic regression model with L2 regularization\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.linear(x))\n",
    "        return out\n",
    "\n",
    "input_dim = 575\n",
    "\n",
    "# Function to train and evaluate the model with L2 regularization\n",
    "def train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5):\n",
    "    model = LogisticRegressionModel(input_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy, model\n",
    "\n",
    "# Outer 10-Fold Cross-Validation\n",
    "outer_kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "outer_accuracies = []\n",
    "best_model_params = None\n",
    "best_outer_accuracy = 0\n",
    "\n",
    "for outer_train_index, outer_test_index in outer_kf.split(remaining_indices):\n",
    "    outer_train_subset = Subset(remaining_set, outer_train_index)\n",
    "    outer_test_subset = Subset(remaining_set, outer_test_index)\n",
    "    \n",
    "    # Inner 10-Fold Cross-Validation\n",
    "    inner_kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    inner_accuracies = []\n",
    "\n",
    "    for inner_train_index, inner_val_index in inner_kf.split(outer_train_index):\n",
    "        inner_train_indices = np.array(outer_train_index)[inner_train_index]\n",
    "        inner_val_indices = np.array(outer_train_index)[inner_val_index]\n",
    "        \n",
    "        inner_train_subset = Subset(remaining_set, inner_train_indices)\n",
    "        inner_val_subset = Subset(remaining_set, inner_val_indices)\n",
    "        \n",
    "        train_loader = DataLoader(inner_train_subset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(inner_val_subset, batch_size=64, shuffle=False)\n",
    "        \n",
    "        accuracy, model = train_and_evaluate_model(train_loader, val_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5)\n",
    "        inner_accuracies.append(accuracy)\n",
    "        print(f'Inner Fold Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    mean_inner_accuracy = np.mean(inner_accuracies)\n",
    "    print(f'Outer Fold Mean Inner Accuracy: {mean_inner_accuracy * 100:.2f}%')\n",
    "\n",
    "    train_loader = DataLoader(outer_train_subset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(outer_test_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    outer_accuracy, outer_model = train_and_evaluate_model(train_loader, test_loader, input_dim, num_epochs=10, lr=0.01, weight_decay=1e-5)\n",
    "    outer_accuracies.append(outer_accuracy)\n",
    "    print(f'Outer Fold Accuracy: {outer_accuracy * 100:.2f}%')\n",
    "\n",
    "    if outer_accuracy > best_outer_accuracy:\n",
    "        best_outer_accuracy = outer_accuracy\n",
    "        best_model_params = outer_model.state_dict()\n",
    "\n",
    "print(f'Mean Outer Accuracy: {np.mean(outer_accuracies) * 100:.2f}%')\n",
    "print('Best model parameters:', best_model_params)\n",
    "\n",
    "# Evaluate the final model on the holdout set\n",
    "final_model = LogisticRegressionModel(input_dim)\n",
    "final_model.load_state_dict(best_model_params)\n",
    "holdout_loader = DataLoader(holdout_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate on holdout set\n",
    "final_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in holdout_loader:\n",
    "        outputs = final_model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "holdout_accuracy = correct / total\n",
    "print(f'Holdout Set Accuracy: {holdout_accuracy * 100:.2f}%')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
