{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titles = ['ADT' ,  'AHM' ,  'AoR' ,  'OE1O' , 'Anæstesidata' , 'Anæstesihændelse' ,  'Blødning' ,  'ca_vitale_værdier' ,   'Dialyse' ,  'EKG_data' ,  'Ekkokardiografi' ,  'Højde' ,  'intellispace' ,  'ITA' ,  'KAG_data' ,  'medicin' ,  'Pleuradræn' ,  'Populationen' ,  'Problemliste' , 'Respirator_data' ,  'Samlet_udskillelse' ,  'Spiromitri' ,  'Total_indgift' ,  'Urin' ,  'Vægt' ,  'viewpoint' ]\n",
    "\n",
    "for title in titles:\n",
    "    parquet_file = f'Processed_parquet_real\\{title}.parquet.gzip'\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    globals()[title] = df\n",
    "\n",
    "Spiromitri.drop(index=[2685,2686,2687,2688],inplace=True) # removes the rows with / in the values\n",
    "Lab_svar = pd.read_csv(r'opdateret_data\\Lab_svar.csv')\n",
    "Lab_svar_1=Lab_svar[:2830372]\n",
    "Lab_svar_2 = Lab_svar[2830372:]\n",
    "\n",
    "Lab_svar_1 = Lab_svar_1.rename(columns={ 'V2' : 'What is being measured' ,  'V3' :  'Timestamp' ,  'V4' :  'Results' })\n",
    "Lab_svar_1 = Lab_svar_1.dropna(subset=['Results'])\n",
    "Lab_svar_1['Results'] = pd.to_numeric(Lab_svar_1['Results'], errors='coerce')\n",
    "Lab_svar_1 = Lab_svar_1.groupby(['Timestamp','IDno','What is being measured']).Results.mean().reset_index()\n",
    "Lab_svar_1 = Lab_svar_1.pivot(index = ['IDno','Timestamp'], columns=['What is being measured'],values='Results')\n",
    "Lab_svar_1.reset_index(inplace=True)\n",
    "Lab_svar_1['Timestamp'] = pd.to_datetime(Lab_svar_1['Timestamp'])\n",
    "\n",
    "Lab_svar_2 = Lab_svar_2.rename(columns={ 'V2' : 'What is being measured' ,  'V3' :  'Timestamp' ,  'V4' :  'Results' })\n",
    "Lab_svar_2 = Lab_svar_2.dropna(subset=['Results'])\n",
    "Lab_svar_2['Results'] = pd.to_numeric(Lab_svar_2['Results'], errors='coerce')\n",
    "Lab_svar_2 = Lab_svar_2.groupby(['Timestamp','IDno','What is being measured']).Results.mean().reset_index()\n",
    "Lab_svar_2 = Lab_svar_2.pivot(index = ['IDno','Timestamp'], columns=['What is being measured'],values='Results')\n",
    "Lab_svar_2.reset_index(inplace=True)\n",
    "Lab_svar_2['Timestamp'] = pd.to_datetime(Lab_svar_2['Timestamp'])\n",
    "\n",
    "Spiromitri=Spiromitri.rename(columns={\"Resultatdato\": \"Timestamp\"})\n",
    "\n",
    "pre_liste = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fjerner alle rækkerne hvor Induktion står lige efter Stop data indsamling da de derfor ikke indgår i åben hjertekirugi\n",
    "test_hændelse = Anæstesihændelse.copy()\n",
    "\n",
    "rows_to_remove = []\n",
    "for i in range(len(test_hændelse) - 1):\n",
    "    if test_hændelse['Hændelse'].iloc[i] == 'Induktion' and test_hændelse['Hændelse'].iloc[i + 1] == 'Stop Data Indsamling':\n",
    "        rows_to_remove.extend([i, i + 1])\n",
    "\n",
    "filtered_test_hændelse = test_hændelse.drop(rows_to_remove).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Populationen and filtered_test_hændelse are DataFrames\n",
    "IDs_listx = Populationen['IDno'].unique()\n",
    "\n",
    "no_induktion = []\n",
    "no_stop = []\n",
    "nothing = []\n",
    "more_induktion = []\n",
    "more_stop = []\n",
    "missing_events = []\n",
    "both = []\n",
    "for ID in IDs_listx:\n",
    "    # Filter the rows where ID matches and Hændelse is 'Induktion' or 'Stop Data Indsamling'\n",
    "    event_rows_induk = filtered_test_hændelse[(filtered_test_hændelse['IDno'] == ID) & (filtered_test_hændelse['Hændelse'] == 'Induktion')]\n",
    "    event_rows_stop = filtered_test_hændelse[(filtered_test_hændelse['IDno'] == ID) & (filtered_test_hændelse['Hændelse'] == 'Stop Data Indsamling')]\n",
    "    event_rows_bypass_start = filtered_test_hændelse[(filtered_test_hændelse['IDno'] == ID) & (filtered_test_hændelse['Hændelse'] == 'CV Bypass Start')]\n",
    "    event_rows_aorta_tang_på = filtered_test_hændelse[(filtered_test_hændelse['IDno'] == ID) & (filtered_test_hændelse['Hændelse'] == 'Aorta tang på')]\n",
    "    event_rows_aorta_tang_af = filtered_test_hændelse[(filtered_test_hændelse['IDno'] == ID) & (filtered_test_hændelse['Hændelse'] == 'Aorta tang af')]\n",
    "    event_rows_bypass_slut = filtered_test_hændelse[(filtered_test_hændelse['IDno'] == ID) & (filtered_test_hændelse['Hændelse'] == 'CV Bypass slut')]\n",
    "\n",
    "    # Check if any event is missing\n",
    "    if (event_rows_bypass_start.empty or event_rows_aorta_tang_på.empty or event_rows_aorta_tang_af.empty or event_rows_bypass_slut.empty):\n",
    "        missing_events.append(ID)\n",
    "\n",
    "    if len(event_rows_induk) > 1 and len(event_rows_stop) > 1:\n",
    "        both.append(ID)\n",
    "\n",
    "    # Check for more than one row\n",
    "    if  len(event_rows_induk) > 1:\n",
    "        if ID not in both:\n",
    "            more_induktion.append(ID)\n",
    "    if len(event_rows_stop) > 1:\n",
    "        if ID not in both:\n",
    "            more_stop.append(ID)\n",
    "\n",
    "    # If it has nothing\n",
    "    if event_rows_induk.empty and event_rows_stop.empty:\n",
    "        nothing.append(ID)\n",
    "    \n",
    "    # Add ID if it doesn't have either 'Induktion' or 'Stop Data Indsamling'\n",
    "    if event_rows_induk.empty:\n",
    "        if ID not in nothing:\n",
    "            no_induktion.append(ID)\n",
    "    if event_rows_stop.empty:\n",
    "        if ID not in nothing:\n",
    "            no_stop.append(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'ADT': ADT,\n",
    "    'AHM': AHM,\n",
    "    'AoR': AoR,\n",
    "    'OE1O': OE1O,\n",
    "    'Anæstesidata': Anæstesidata,\n",
    "    'Anæstesihændelse': filtered_test_hændelse,\n",
    "    'Blødning': Blødning,\n",
    "    'ca_vitale_værdier': ca_vitale_værdier,\n",
    "    'Dialyse': Dialyse,\n",
    "    'EKG_data': EKG_data,\n",
    "    'Ekkokardiografi': Ekkokardiografi,\n",
    "    'Højde': Højde,\n",
    "    'intellispace': intellispace,\n",
    "    'ITA': ITA,\n",
    "    'KAG_data': KAG_data,\n",
    "    'Lab_svar': Lab_svar,\n",
    "    'medicin': medicin,\n",
    "    'Pleuradræn': Pleuradræn,\n",
    "    'Populationen': Populationen,\n",
    "    'Problemliste': Problemliste,\n",
    "    'Respirator_data': Respirator_data,\n",
    "    'Samlet_udskillelse': Samlet_udskillelse,\n",
    "    'Spiromitri': Spiromitri,\n",
    "    'Total_indgift': Total_indgift,\n",
    "    'Urin': Urin,\n",
    "    'Vægt': Vægt,\n",
    "    'viewpoint': viewpoint\n",
    "}\n",
    "\n",
    "def check_timestamp_header_and_type(df_dict):\n",
    "    results = []\n",
    "    for name, df in df_dict.items():\n",
    "        if 'Timestamp' in df.columns:\n",
    "            timestamp_type = df['Timestamp'].dtype\n",
    "            if timestamp_type == np.dtype('O'):\n",
    "                try:\n",
    "                    # Try parsing with the default format\n",
    "                    df['Timestamp'] = pd.to_datetime(df['Timestamp'], utc=True)\n",
    "                except Exception as e_default:\n",
    "                    print(f\"Default parsing failed for DataFrame '{name}' with error: {e_default}\")\n",
    "                    try:\n",
    "                        # Try parsing with dayfirst=True\n",
    "                        df['Timestamp'] = pd.to_datetime(df['Timestamp'], utc=True, dayfirst=True)\n",
    "                    except Exception as e_dayfirst:\n",
    "                        print(f\"Parsing with dayfirst=True failed for DataFrame '{name}' with error: {e_dayfirst}\")\n",
    "                        try:\n",
    "                            # If parsing fails, you can specify a custom format or handle mixed formats\n",
    "                            # Example: df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d-%m-%Y', utc=True)\n",
    "                            df['Timestamp'] = pd.to_datetime(df['Timestamp'], utc=True, errors='coerce')\n",
    "                            if df['Timestamp'].isnull().any():\n",
    "                                raise ValueError(\"Some dates could not be parsed and resulted in NaT values.\")\n",
    "                        except Exception as e_custom:\n",
    "                            print(f\"Custom parsing failed for DataFrame '{name}' with error: {e_custom}\")\n",
    "                            results.append((name, False, None))\n",
    "                            continue\n",
    "                timestamp_type = df['Timestamp'].dtype  # Update type after conversion\n",
    "            results.append((name, True, timestamp_type))\n",
    "        else:\n",
    "            results.append((name, False, None))\n",
    "    return results\n",
    "\n",
    "# Check each DataFrame in the dictionary for the 'Timestamp' header and its type\n",
    "results = check_timestamp_header_and_type(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs_listx = np.unique(Populationen['IDno'])\n",
    "IDs_listx = [ID for ID in IDs_listx if ID not in no_induktion and ID not in no_stop and ID not in nothing and ID not in more_induktion and ID not in more_stop and ID not in missing_events and ID not in both]\n",
    "IDS_dfx = pd.DataFrame({'IDno': IDs_listx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anæstesihændelse_clean = filtered_test_hændelse\n",
    "for id in IDs_listx:\n",
    "    Anæstesihændelse_id = filtered_test_hændelse[filtered_test_hændelse['IDno'] == id]\n",
    "    \n",
    "    # Initialize dictionaries to store indexes of rows to keep\n",
    "    first_instances = {}\n",
    "    last_instances = {}\n",
    "    \n",
    "    # Initialize a list to store indexes of rows to remove\n",
    "    rows_to_remove = []\n",
    "\n",
    "    # Loop through the dataframe to decide which rows to keep or remove\n",
    "    for index, row in Anæstesihændelse_id.iterrows():\n",
    "        \n",
    "        event = row['Hændelse']\n",
    "        \n",
    "        # Mark the first instances\n",
    "        if event == 'Induktion' and 'Induktion' not in first_instances:\n",
    "            first_instances['Induktion'] = index\n",
    "        if event == 'CV Bypass Start' and 'CV Bypass Start' not in first_instances:\n",
    "            first_instances['CV Bypass Start'] = index\n",
    "        if event == 'Aorta tang på' and 'Aorta tang på' not in first_instances:\n",
    "            first_instances['Aorta tang på'] = index\n",
    "        \n",
    "        # Continuously update the last instances\n",
    "        if event == 'Aorta tang af':\n",
    "            last_instances['Aorta tang af'] = index\n",
    "        if event == 'CV Bypass slut':\n",
    "            last_instances['CV Bypass slut'] = index\n",
    "        if event == 'Stop Data Indsamling':\n",
    "            last_instances['Stop Data Indsamling'] = index\n",
    "    \n",
    "    # Create a set of indexes to keep\n",
    "    indexes_to_keep = set(first_instances.values()).union(last_instances.values())\n",
    "    \n",
    "    # Mark rows to remove\n",
    "    for index in Anæstesihændelse_id.index:\n",
    "        if index not in indexes_to_keep:\n",
    "            rows_to_remove.append(index)\n",
    "\n",
    "\n",
    "    Anæstesihændelse_clean.drop(rows_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load filer. OBS! Dialyse fase 1-4 gemmer ikke, fordi deres dataframes er tomme. Processed versioner er gemt, så det er den, der skal loades i stedet for. Hvis processed versioner er lavet, så load dem i stedet for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"Phases\"\n",
    "\n",
    "# Define the base file names\n",
    "base_file_names = [\n",
    "    # \"PhasesAHM_Phase_\",\n",
    "    # \"PhasesAnæstesidata_Phase_\",\n",
    "    # \"PhasesBlødning_Phase_\",\n",
    "    # \"PhasesLab_svar_1_Phase_\",\n",
    "    # \"PhasesLab_svar_2_Phase_\",\n",
    "    \"PhasesTotal_indgift_Phase_\"\n",
    "]\n",
    "\n",
    "# \"Dialyse_Phase_\",\n",
    "\n",
    "def LoadData(folder_path,base_file_names):\n",
    "    # Initialize a dictionary to store DataFrames for all phases\n",
    "    dataframes = {phase: {} for phase in range(1, 6)}\n",
    "\n",
    "    # Loop through each phase and each base file name to load the CSV files\n",
    "    for phase in range(1, 6):\n",
    "        for base_file_name in base_file_names:\n",
    "            file_path = f\"{folder_path}/{base_file_name}{phase}.csv\"\n",
    "            key = base_file_name.split(\"_Phase_\")[0] + f\"_Phase_{phase}\"\n",
    "            dataframes[phase][key] = pd.read_csv(file_path)\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = LoadData(folder_path=folder_path,base_file_names=base_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_indgift_Phase_1 = df[1]['PhasesTotal_indgift_Phase_1']\n",
    "Total_indgift_Phase_2 = df[2]['PhasesTotal_indgift_Phase_2']\n",
    "Total_indgift_Phase_3 = df[3]['PhasesTotal_indgift_Phase_3']\n",
    "Total_indgift_Phase_4 = df[4]['PhasesTotal_indgift_Phase_4']\n",
    "Total_indgift_Phase_5 = df[5]['PhasesTotal_indgift_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blødning_Phase_1 = df[1]['PhasesBlødning_Phase_1']\n",
    "Blødning_Phase_2 = df[2]['PhasesBlødning_Phase_2']\n",
    "Blødning_Phase_3 = df[3]['PhasesBlødning_Phase_3']\n",
    "Blødning_Phase_4 = df[4]['PhasesBlødning_Phase_4']\n",
    "Blødning_Phase_5 = df[5]['PhasesBlødning_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anæstesidata_Phase_1 = df[1]['PhasesAnæstesidata_Phase_1']\n",
    "Anæstesidata_Phase_2 = df[2]['PhasesAnæstesidata_Phase_2']\n",
    "Anæstesidata_Phase_3 = df[3]['PhasesAnæstesidata_Phase_3']\n",
    "Anæstesidata_Phase_4 = df[4]['PhasesAnæstesidata_Phase_4']\n",
    "Anæstesidata_Phase_5 = df[5]['PhasesAnæstesidata_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab_svar_1_Phase_1 = df[1]['PhasesLab_svar_1_Phase_1']\n",
    "Lab_svar_1_Phase_2 = df[2]['PhasesLab_svar_1_Phase_2']\n",
    "Lab_svar_1_Phase_3 = df[3]['PhasesLab_svar_1_Phase_3']\n",
    "Lab_svar_1_Phase_4 = df[4]['PhasesLab_svar_1_Phase_4']\n",
    "Lab_svar_1_Phase_5 = df[5]['PhasesLab_svar_1_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lab_svar_2_Phase_1 = df[1]['PhasesLab_svar_2_Phase_1']\n",
    "Lab_svar_2_Phase_2 = df[2]['PhasesLab_svar_2_Phase_2']\n",
    "Lab_svar_2_Phase_3 = df[3]['PhasesLab_svar_2_Phase_3']\n",
    "Lab_svar_2_Phase_4 = df[4]['PhasesLab_svar_2_Phase_4']\n",
    "Lab_svar_2_Phase_5 = df[5]['PhasesLab_svar_2_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_Phase_1 = df[1]['PhasesAHM_Phase_1']\n",
    "AHM_Phase_2 = df[2]['PhasesAHM_Phase_2']\n",
    "AHM_Phase_3 = df[3]['PhasesAHM_Phase_3']\n",
    "AHM_Phase_4 = df[4]['PhasesAHM_Phase_4']\n",
    "AHM_Phase_5 = df[5]['PhasesAHM_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Phases'\n",
    "base_file_names = ['Processed_AHM_Phase_']\n",
    "df_AHM = LoadData(folder_path,base_file_names)\n",
    "\n",
    "Processed_AHM_Phase_1 = df_AHM[1]['Processed_AHM_Phase_1']\n",
    "Processed_AHM_Phase_2 = df_AHM[2]['Processed_AHM_Phase_2']\n",
    "Processed_AHM_Phase_3 = df_AHM[3]['Processed_AHM_Phase_3']\n",
    "Processed_AHM_Phase_4 = df_AHM[4]['Processed_AHM_Phase_4']\n",
    "Processed_AHM_Phase_5 = df_AHM[5]['Processed_AHM_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Phases'\n",
    "base_file_names = ['Processed_Dialyse_Phase_']\n",
    "df_Dialyse = LoadData(folder_path,base_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Dialyse_Phase_1 = df_Dialyse[1]['Processed_Dialyse_Phase_1']\n",
    "Processed_Dialyse_Phase_2 = df_Dialyse[2]['Processed_Dialyse_Phase_2']\n",
    "Processed_Dialyse_Phase_3 = df_Dialyse[3]['Processed_Dialyse_Phase_3']\n",
    "Processed_Dialyse_Phase_4 = df_Dialyse[4]['Processed_Dialyse_Phase_4']\n",
    "Processed_Dialyse_Phase_5 = df_Dialyse[5]['Processed_Dialyse_Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Anæstesihændelse_clean['IDno'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funky_ids = []\n",
    "# for id in IDs_listx:\n",
    "#     eeeeet = Anæstesihændelse_clean[Anæstesihændelse_clean['IDno'] == id]\n",
    "#     counts = {}\n",
    "#     for value in eeeeet['Hændelse']:\n",
    "#         counts[value] = counts.get(value, 0) + 1\n",
    "#     keys = list(counts.keys())\n",
    "#     for key in keys:\n",
    "#         if counts[key] != 1:\n",
    "#             #print(f'{key} occurs {counts[key]} times for {id}')\n",
    "#             funky_ids.append(id)\n",
    "\n",
    "# funky_ids=list(np.unique(funky_ids))\n",
    "# IDs_listx = [ID for ID in IDs_listx if ID not in funky_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phases(IDno,Dataframe): # Har indtil videre brugt AHM_processed i stedet for Dataframe\n",
    "    Hændelsestidspunkt = Anæstesihændelse_clean[Anæstesihændelse_clean['IDno'] == IDno]\n",
    "\n",
    "    Induktion = Hændelsestidspunkt[Hændelsestidspunkt['Hændelse'] == 'Induktion']['Timestamp'].reset_index(drop = True)[0]\n",
    "    Bypass_start = (Hændelsestidspunkt[Hændelsestidspunkt['Hændelse'] == 'CV Bypass Start']['Timestamp']).reset_index(drop = True)[0]\n",
    "    Aorta_tang_på = Hændelsestidspunkt[Hændelsestidspunkt['Hændelse'] == 'Aorta tang på']['Timestamp'].reset_index(drop = True)[0]\n",
    "    Aorta_tang_af = Hændelsestidspunkt[Hændelsestidspunkt['Hændelse'] == 'Aorta tang af']['Timestamp'].reset_index(drop = True)[0]\n",
    "    Bypass_slut = Hændelsestidspunkt[Hændelsestidspunkt['Hændelse'] == 'CV Bypass slut']['Timestamp'].reset_index(drop = True)[0]\n",
    "    Stop_indsaml = Hændelsestidspunkt[Hændelsestidspunkt['Hændelse'] == 'Stop Data Indsamling']['Timestamp'].reset_index(drop = True)[0]\n",
    "    AHM_ID = Dataframe[Dataframe['IDno'] == IDno]\n",
    "    AHM_ID = AHM_ID.set_index(['Timestamp'])\n",
    "\n",
    "    Phase_1 = AHM_ID.loc[Induktion:(Bypass_start)]\n",
    "    Phase_1 = Phase_1.drop(Phase_1.index[-1])\n",
    "\n",
    "    Phase_2 = AHM_ID.loc[Bypass_start:(Aorta_tang_på)]\n",
    "    Phase_2 = Phase_2.drop(Phase_2.index[-1])\n",
    "\n",
    "    Phase_3 = AHM_ID.loc[Aorta_tang_på:(Aorta_tang_af)]\n",
    "    Phase_3 = Phase_3.drop(Phase_3.index[-1])\n",
    "\n",
    "    Phase_4 = AHM_ID.loc[Aorta_tang_af:(Bypass_slut)]\n",
    "    Phase_4 = Phase_4.drop(Phase_4.index[-1])\n",
    "\n",
    "    Phase_5 = AHM_ID.loc[Bypass_slut:(Stop_indsaml)]\n",
    "    return Phase_1, Phase_2,Phase_3,Phase_4,Phase_5\n",
    "\n",
    "def Aggregate_Phases(id_list, Dataframe):\n",
    "    phase_1_list = []\n",
    "    phase_2_list = []\n",
    "    phase_3_list = []\n",
    "    phase_4_list = []\n",
    "    phase_5_list = []\n",
    "\n",
    "    for IDno in id_list:\n",
    "        print(IDno)\n",
    "        Phase_1, Phase_2, Phase_3, Phase_4, Phase_5 = Phases(IDno, Dataframe)\n",
    "        phase_1_list.append(Phase_1)\n",
    "        phase_2_list.append(Phase_2)\n",
    "        phase_3_list.append(Phase_3)\n",
    "        phase_4_list.append(Phase_4)\n",
    "        phase_5_list.append(Phase_5)\n",
    "\n",
    "    phase_1_df = pd.concat(phase_1_list)\n",
    "    phase_2_df = pd.concat(phase_2_list)\n",
    "    phase_3_df = pd.concat(phase_3_list)\n",
    "    phase_4_df = pd.concat(phase_4_list)\n",
    "    phase_5_df = pd.concat(phase_5_list)\n",
    "\n",
    "    return phase_1_df, phase_2_df, phase_3_df, phase_4_df, phase_5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_ids_and_timestamp(IDs_listx, df):\n",
    "    phase_dfs = {\n",
    "        'Phase_1': [],\n",
    "        'Phase_2': [],\n",
    "        'Phase_3': [],\n",
    "        'Phase_4': [],\n",
    "        'Phase_5': []\n",
    "    }\n",
    "\n",
    "    for idno in IDs_listx:\n",
    "        #print(idno)\n",
    "        # Filter Anæstesihændelse based on ID number\n",
    "        hændelse_data = Anæstesihændelse_clean[Anæstesihændelse_clean['IDno'] == idno]\n",
    "\n",
    "        if not hændelse_data.empty:\n",
    "            # Extract relevant timestamps\n",
    "            Induktion_time = hændelse_data[hændelse_data['Hændelse'] == 'Induktion']['Timestamp'].values\n",
    "            Bypass_start_time = hændelse_data[hændelse_data['Hændelse'] == 'CV Bypass Start']['Timestamp'].values\n",
    "            Aorta_tang_på_time = hændelse_data[hændelse_data['Hændelse'] == 'Aorta tang på']['Timestamp'].values\n",
    "            Aorta_tang_af_time = hændelse_data[hændelse_data['Hændelse'] == 'Aorta tang af']['Timestamp'].values\n",
    "            Bypass_slut_time = hændelse_data[hændelse_data['Hændelse'] == 'CV Bypass slut']['Timestamp'].values\n",
    "            Stop_indsaml_time = hændelse_data[hændelse_data['Hændelse'] == 'Stop Data Indsamling']['Timestamp'].values\n",
    "\n",
    "            if len(Induktion_time) > 0 and len(Bypass_start_time) > 0 and len(Aorta_tang_på_time) > 0 and len(Aorta_tang_af_time) > 0 and len(Bypass_slut_time) > 0 and len(Stop_indsaml_time) > 0:\n",
    "                current_df = df[df['IDno'] == idno]\n",
    "                Induktion_time_utc = pd.Timestamp(Induktion_time[0], tz='UTC')\n",
    "                Bypass_start_time_utc = pd.Timestamp(Bypass_start_time[0], tz='UTC')\n",
    "                Aorta_tang_på_time_utc = pd.Timestamp(Aorta_tang_på_time[0], tz='UTC')\n",
    "                Aorta_tang_af_time_utc = pd.Timestamp(Aorta_tang_af_time[0], tz='UTC')\n",
    "                Bypass_slut_time_utc = pd.Timestamp(Bypass_slut_time[0], tz='UTC')\n",
    "                Stop_indsaml_time_utc = pd.Timestamp(Stop_indsaml_time[0], tz='UTC')\n",
    "\n",
    "                # Phase 2: Filter between Induktion and Bypass_start\n",
    "                phase_1 = current_df[(current_df['Timestamp'] >= Induktion_time_utc) & (current_df['Timestamp'] < Bypass_start_time_utc)]\n",
    "                phase_dfs['Phase_1'].append(phase_1)\n",
    "\n",
    "                # Phase 3: Filter between Bypass_start and Aorta_tang_på\n",
    "                phase_2 = current_df[(current_df['Timestamp'] >= Bypass_start_time_utc) & (current_df['Timestamp'] < Aorta_tang_på_time_utc)]\n",
    "                phase_dfs['Phase_2'].append(phase_2)\n",
    "\n",
    "                # Phase 4: Filter between Aorta_tang_på and Aorta_tang_af\n",
    "                phase_3 = current_df[(current_df['Timestamp'] >= Aorta_tang_på_time_utc) & (current_df['Timestamp'] < Aorta_tang_af_time_utc)]\n",
    "                phase_dfs['Phase_3'].append(phase_3)\n",
    "\n",
    "                # Phase 5: Filter between Aorta_tang_af and Bypass_slut\n",
    "                phase_4 = current_df[(current_df['Timestamp'] >= Aorta_tang_af_time_utc) & (current_df['Timestamp'] < Bypass_slut_time_utc)]\n",
    "                phase_dfs['Phase_4'].append(phase_4)\n",
    "\n",
    "                # Phase 6: Filter between Bypass_slut and Stop_indsaml\n",
    "                phase_5 = current_df[(current_df['Timestamp'] >= Bypass_slut_time_utc) & (current_df['Timestamp'] <= Stop_indsaml_time_utc)]\n",
    "                phase_dfs['Phase_5'].append(phase_5)\n",
    "\n",
    "    # Concatenate all phases\n",
    "    for phase_name in phase_dfs:\n",
    "        phase_dfs[phase_name] = pd.concat(phase_dfs[phase_name]) if phase_dfs[phase_name] else pd.DataFrame()\n",
    "\n",
    "    return phase_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fejlkilde hvis data indsamling først er blevet stoppet offficielt lang tid efter operationen kan det påvirke vores data. Kan tage gennemsnit og så afslutte operationer konsekvent et par timer efter CV bypass slut, dette kan nok også gøres for dem der mangler en stop data indsamling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames\n",
    "Dataframes_intra = [AHM, Anæstesidata, Blødning, Dialyse,Lab_svar_1, Lab_svar_2, Total_indgift]\n",
    "\n",
    "# List of DataFrame names\n",
    "df_names = ['AHM', 'Anæstesidata', 'Blødning', 'Dialyse','Lab_svar_1', 'Lab_svar_2', 'Total_indgift']\n",
    "\n",
    "# Unique IDs from Populationen and filter them\n",
    "\n",
    "# Initialize a dictionary to store filtered DataFrames\n",
    "filtered_dataframes = {}\n",
    "\n",
    "# Process each DataFrame in Dataframes_pre\n",
    "for df_name, df in zip(df_names, Dataframes_intra):\n",
    "    # Ensure the DataFrame has the 'IDno' column\n",
    "    print(df_name)\n",
    "    if 'IDno' in df.columns:\n",
    "        # Apply the filtering function to each DataFrame\n",
    "        filtered_df = filter_dataframe_by_ids_and_timestamp(IDs_listx, df)\n",
    "        filtered_dataframes[df_name] = filtered_df\n",
    "    else:\n",
    "        print(f\"DataFrame {df_name} does not contain 'IDno' column.\")\n",
    "\n",
    "# Validate if the length of unique IDs in filtered_Population matches IDs_listx\n",
    "if 'Populationen' in filtered_dataframes:\n",
    "    unique_ids_in_filtered_population = filtered_dataframes['Populationen']['IDno'].nunique()\n",
    "    if unique_ids_in_filtered_population != len(IDs_listx):\n",
    "        print(f\"Mismatch: {unique_ids_in_filtered_population} unique IDs in filtered_Population, expected {len(IDs_listx)}.\")\n",
    "\n",
    "# Extract the filtered DataFrames back to individual variables if needed\n",
    "filtered_AHM =filtered_dataframes.get('AHM',None)\n",
    "filtered_Anæstesidata =filtered_dataframes.get('Anæstesidata',None)\n",
    "# filtered_Anæstesihændelse =filtered_dataframes.get('Anæstesihændelse',None)\n",
    "filtered_Blødning =filtered_dataframes.get('Blødning',None)\n",
    "filtered_Dialyse =filtered_dataframes.get('Dialyse',None)\n",
    "filtered_Lab_svar_1 =filtered_dataframes.get('Lab_svar_1',None)\n",
    "filtered_Lab_svar_2 =filtered_dataframes.get('Lab_svar_2',None)\n",
    "filtered_Total_indgift =filtered_dataframes.get('Total_indgift',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_Phase_1 = filtered_AHM['Phase_1']\n",
    "Anæstesidata_Phase_1 = filtered_Anæstesidata['Phase_1']\n",
    "Blødning_Phase_1 = filtered_Blødning['Phase_1']\n",
    "Dialyse_Phase_1 = filtered_Dialyse['Phase_1']\n",
    "Lab_svar_1_Phase_1 = filtered_Lab_svar_1['Phase_1']\n",
    "Lab_svar_2_Phase_1 = filtered_Lab_svar_2['Phase_1']\n",
    "Total_indgift_Phase_1 = filtered_Total_indgift['Phase_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_Phase_2 = filtered_AHM['Phase_2']\n",
    "Anæstesidata_Phase_2 = filtered_Anæstesidata['Phase_2']\n",
    "Blødning_Phase_2 = filtered_Blødning['Phase_2']\n",
    "Dialyse_Phase_2 = filtered_Dialyse['Phase_2']\n",
    "Lab_svar_1_Phase_2 = filtered_Lab_svar_1['Phase_2']\n",
    "Lab_svar_2_Phase_2 = filtered_Lab_svar_2['Phase_2']\n",
    "Total_indgift_Phase_2 = filtered_Total_indgift['Phase_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_Phase_3 = filtered_AHM['Phase_3']\n",
    "Anæstesidata_Phase_3 = filtered_Anæstesidata['Phase_3']\n",
    "Blødning_Phase_3 = filtered_Blødning['Phase_3']\n",
    "Dialyse_Phase_3 = filtered_Dialyse['Phase_3']\n",
    "Lab_svar_1_Phase_3 = filtered_Lab_svar_1['Phase_3']\n",
    "Lab_svar_2_Phase_3 = filtered_Lab_svar_2['Phase_3']\n",
    "Total_indgift_Phase_3 = filtered_Total_indgift['Phase_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_Phase_4 = filtered_AHM['Phase_4']\n",
    "Anæstesidata_Phase_4 = filtered_Anæstesidata['Phase_4']\n",
    "Blødning_Phase_4 = filtered_Blødning['Phase_4']\n",
    "Dialyse_Phase_4 = filtered_Dialyse['Phase_4']\n",
    "Lab_svar_1_Phase_4 = filtered_Lab_svar_1['Phase_4']\n",
    "Lab_svar_2_Phase_4 = filtered_Lab_svar_2['Phase_4']\n",
    "Total_indgift_Phase_4 = filtered_Total_indgift['Phase_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_Phase_5 = filtered_AHM['Phase_5']\n",
    "Anæstesidata_Phase_5 = filtered_Anæstesidata['Phase_5']\n",
    "Blødning_Phase_5 = filtered_Blødning['Phase_5']\n",
    "Dialyse_Phase_5 = filtered_Dialyse['Phase_5']\n",
    "Lab_svar_1_Phase_5 = filtered_Lab_svar_1['Phase_5']\n",
    "Lab_svar_2_Phase_5 = filtered_Lab_svar_2['Phase_5']\n",
    "Total_indgift_Phase_5 = filtered_Total_indgift['Phase_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths where you want to save the CSV files\n",
    "output_path_phases = 'Phases'\n",
    "\n",
    "# Function to save phases\n",
    "def save_phases(filtered_data, dataset_name):\n",
    "    for key, phase_df in filtered_data.items():\n",
    "        if not phase_df.empty:\n",
    "            phase_df.to_csv(f\"{output_path_phases}/{dataset_name}_{key}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(IDS_dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save phases for Anæstesidata\n",
    "save_phases(filtered_Anæstesidata, 'Anæstesidata')\n",
    "\n",
    "# Save phases for AHM\n",
    "save_phases(filtered_AHM, 'AHM')\n",
    "\n",
    "# Save phases for Blødning\n",
    "save_phases(filtered_Blødning, 'Blødning')\n",
    "\n",
    "# Save phases for Dialyse\n",
    "save_phases(filtered_Dialyse, 'Dialyse')\n",
    "\n",
    "# Save phases for Lab_svar_1\n",
    "save_phases(filtered_Lab_svar_1, 'Lab_svar_1')\n",
    "\n",
    "# Save phases for Lab_svar_2\n",
    "save_phases(filtered_Lab_svar_2, 'Lab_svar_2')\n",
    "\n",
    "# Save phases for Total_indgift\n",
    "save_phases(filtered_Total_indgift, 'Total_indgift')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dialyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DialyseProcessing(filtered_Dialyse):\n",
    "    Processed_Dialyse = filtered_Dialyse.drop(columns =['Blodflow - indstille','Mål for væsketræk', 'Timestamp']) #Vi sletter mål for væsketræk, da det kun er målt 61 gange\n",
    "    encoded_dia1 = pd.get_dummies(filtered_Dialyse['Filtersæt'])\n",
    "    encoded_dia2 = pd.get_dummies(filtered_Dialyse['Væsketype dialysat'])\n",
    "    encoded_dia3 = pd.get_dummies(filtered_Dialyse['CRRT modus'])\n",
    "    df = pd.concat([Processed_Dialyse, encoded_dia1.astype(int), encoded_dia2.astype(int), encoded_dia3.astype(int)], axis=1)\n",
    "    df = df.drop(columns=['Filtersæt','Væsketype dialysat', 'CRRT modus'])\n",
    "    Processed_Dialyse = df\n",
    "    Processed_Dialyse['Valg af antikoagulant til Multifiltrate: CVVHD eller CVVHDF'] = Processed_Dialyse['Valg af antikoagulant til Multifiltrate: CVVHD eller CVVHDF'].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "    # List of columns to calculate statistics for\n",
    "    columns_of_interest = [\n",
    "        'Dialysatvæske hastighed - indstillet',\n",
    "        'Væskefjernelse fra patienten - indstillet',\n",
    "        'Væskefjernelse fra patienten - aflæst',\n",
    "        'Væsketræk siden sidste aflæsning (I/U)'\n",
    "    ]\n",
    "\n",
    "    # Calculate mean and variance for each \"IDno\" and column\n",
    "    mean_df = Processed_Dialyse.groupby('IDno')[columns_of_interest].mean().add_suffix('_mean')\n",
    "    var_df = Processed_Dialyse.groupby('IDno')[columns_of_interest].var().add_suffix('_var')\n",
    "\n",
    "    # Merge the mean and variance dataframes with the original dataframe\n",
    "    Processed_Dialyse = Processed_Dialyse.merge(mean_df, on='IDno', how='left')\n",
    "    Processed_Dialyse = Processed_Dialyse.merge(var_df, on='IDno', how='left')\n",
    "    Processed_Dialyse = Processed_Dialyse.drop(columns = columns_of_interest)\n",
    "    Processed_Dialyse = Processed_Dialyse.groupby('IDno').max().reset_index()\n",
    "\n",
    "    # Identify IDno values that are in Populationen but not in Processed_Dialyse\n",
    "    missing_idnos = IDS_dfx[~IDS_dfx['IDno'].isin(Processed_Dialyse['IDno'])]['IDno']\n",
    "\n",
    "    # Create a dataframe with these missing IDnos and 0 in every other input\n",
    "    missing_data = pd.DataFrame(missing_idnos, columns=['IDno'])\n",
    "    for col in Processed_Dialyse.columns:\n",
    "        if col != 'IDno':\n",
    "            missing_data[col] = 0\n",
    "\n",
    "    # Append this missing data to Processed_Dialyse\n",
    "    Processed_Dialyse = pd.concat([Processed_Dialyse, missing_data], ignore_index=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    Processed_Dialyse.fillna(0, inplace=True)\n",
    "    Processed_Dialyse = Processed_Dialyse.groupby('IDno').max().reset_index()\n",
    "    pre_liste.append(Processed_Dialyse)\n",
    "    return Processed_Dialyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Dialyse_Phase_1 = DialyseProcessing(Dialyse_Phase_1)\n",
    "Processed_Dialyse_Phase_2 = DialyseProcessing(Dialyse_Phase_2)\n",
    "Processed_Dialyse_Phase_3 = DialyseProcessing(Dialyse_Phase_3)\n",
    "Processed_Dialyse_Phase_4 = DialyseProcessing(Dialyse_Phase_4)\n",
    "Processed_Dialyse_Phase_5 = DialyseProcessing(Dialyse_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_phase(output_path_phases,dataset_name,phase_df):\n",
    "    phase_df.to_csv(f\"{output_path_phases}/{dataset_name}_{key}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Dialyse_Phase_1.to_csv(\"Processed_Dialyse_Phase_1.csv\", index=False)\n",
    "Processed_Dialyse_Phase_2.to_csv(\"Processed_Dialyse_Phase_2.csv\", index=False)\n",
    "Processed_Dialyse_Phase_3.to_csv(\"Processed_Dialyse_Phase_3.csv\", index=False)\n",
    "Processed_Dialyse_Phase_4.to_csv(\"Processed_Dialyse_Phase_4.csv\", index=False)\n",
    "Processed_Dialyse_Phase_5.to_csv(\"Processed_Dialyse_Phase_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labsvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lab_svarProcessing1(filtered_Lab_svar_1,filtered_Lab_svar_2):\n",
    "    # print(filtered_Lab_svar_1)\n",
    "    ja = filtered_Lab_svar_1.drop(columns = 'Timestamp')\n",
    "    # print(\"ja\")\n",
    "    # print(ja)\n",
    "    grouped = ja.groupby('IDno')\n",
    "    # print(\"Grouped:\")\n",
    "    # print(grouped)\n",
    "    agg_dict = {col: ['mean', 'var'] for col in ja.columns if col != 'IDno'}\n",
    "    # print(\"agg_dict:\")\n",
    "    # print(agg_dict)\n",
    "    result = grouped.agg(agg_dict)\n",
    "    # print(\"Result:\")\n",
    "    # print(result)\n",
    "    result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "    for col in result.columns:\n",
    "        if 'var' in col:\n",
    "            mean_col = col.replace('var', 'mean')\n",
    "            result[col] = result.apply(lambda row: 0 if pd.notna(row[mean_col]) and pd.isna(row[col]) else row[col], axis=1)\n",
    "    Processed_Lab_svar_1 = result\n",
    "    # print(\"Processed lab svar 1:\")\n",
    "    # print(Processed_Lab_svar_1)\n",
    "\n",
    "    ja = filtered_Lab_svar_2.drop(columns = 'Timestamp')\n",
    "    grouped = ja.groupby('IDno')\n",
    "    agg_dict = {col: ['mean', 'var'] for col in ja.columns if col != 'IDno'}\n",
    "    result = grouped.agg(agg_dict)\n",
    "    result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "    for col in result.columns:\n",
    "        if 'var' in col:\n",
    "            mean_col = col.replace('var', 'mean')\n",
    "            result[col] = result.apply(lambda row: 0 if pd.notna(row[mean_col]) and pd.isna(row[col]) else row[col], axis=1)\n",
    "    for col in result.columns:\n",
    "        if 'mean' in col:\n",
    "            var_col = col.replace('mean', 'var')\n",
    "            valid_col = col.replace('mean', 'valid')\n",
    "            result[valid_col] = result.apply(lambda row: pd.notna(row[col]) and pd.notna(row[var_col]), axis=1)\n",
    "    Processed_Lab_svar_2 = result\n",
    "\n",
    "    kol_navn = []\n",
    "    for i in range(len(Processed_Lab_svar_1.columns)):\n",
    "        if list(Processed_Lab_svar_1.columns)[i] not in kol_navn:\n",
    "            kol_navn.append(list(Processed_Lab_svar_1.columns)[i])\n",
    "    for i in range(len(Lab_svar_2.columns)):\n",
    "        if list(Processed_Lab_svar_2.columns)[i] not in kol_navn:\n",
    "            kol_navn.append(list(Processed_Lab_svar_2.columns)[i])\n",
    "    kol_navn = kol_navn[2:]\n",
    "    # print(\"Kol navn:\")\n",
    "    # print(kol_navn)\n",
    "    ny_list = {}\n",
    "    for i in range(len(kol_navn)):\n",
    "        tal = 0\n",
    "        if kol_navn[i] in Processed_Lab_svar_1:\n",
    "            tal = tal + (len(Processed_Lab_svar_1[~Processed_Lab_svar_1[kol_navn[i]].isna()]))\n",
    "        if kol_navn[i] in Processed_Lab_svar_2:\n",
    "            tal = tal + (len(Processed_Lab_svar_2[~Processed_Lab_svar_2[kol_navn[i]].isna()]))\n",
    "        ny_list[kol_navn[i]] = tal\n",
    "    \n",
    "    # print(\"Ny list:\")\n",
    "    # print(len(ny_list))\n",
    "    # print(ny_list)\n",
    "    return Processed_Lab_svar_1, Processed_Lab_svar_2, kol_navn,ny_list\n",
    "\n",
    "\n",
    "def Lab_svarProcessing2(Processed_Lab_svar_1, Processed_Lab_svar_2, kol_navn,ny_list):\n",
    "    antal = 0\n",
    "    hvilke = []\n",
    "    for i in range(len(ny_list)):\n",
    "        if ny_list[kol_navn[i]] >= 500:\n",
    "            antal += 1\n",
    "            hvilke.append(kol_navn[i])\n",
    "    antal = antal / 2\n",
    "    # print('Hvilke:')\n",
    "    # print(hvilke)\n",
    "\n",
    "    Processed_Lab_svar_1 = Processed_Lab_svar_1[hvilke]\n",
    "    Processed_Lab_svar_2 = Processed_Lab_svar_2[hvilke]\n",
    "    # print(\"Processed_lab_svar_1[hvilke]:\")\n",
    "    # print(Processed_Lab_svar_1)\n",
    "\n",
    "    return Processed_Lab_svar_1, Processed_Lab_svar_2, hvilke, antal\n",
    "\n",
    "def Lab_svarProcessing3(Processed_Lab_svar_1, Processed_Lab_svar_2):\n",
    "    for col in Processed_Lab_svar_1.columns:\n",
    "        if 'mean' in col:\n",
    "            var_col = col.replace('mean', 'var')\n",
    "            valid_col = col.replace('mean', 'valid')\n",
    "            Processed_Lab_svar_1[valid_col] = Processed_Lab_svar_1.apply(lambda row: pd.notna(row[col]) and pd.notna(row[var_col]), axis=1)\n",
    "    for col in Processed_Lab_svar_2.columns:\n",
    "        if 'mean' in col:\n",
    "            var_col = col.replace('mean', 'var')\n",
    "            valid_col = col.replace('mean', 'valid')\n",
    "            Processed_Lab_svar_2[valid_col] = Processed_Lab_svar_2.apply(lambda row: pd.notna(row[col]) and pd.notna(row[var_col]), axis=1)\n",
    "\n",
    "    Processed_Lab_svar_1 = Processed_Lab_svar_1.fillna(0)\n",
    "    Processed_Lab_svar_2 = Processed_Lab_svar_2.fillna(0)\n",
    "    Processed_Lab_svar_1.reset_index(inplace = True)\n",
    "    Processed_Lab_svar_2.reset_index(inplace = True)\n",
    "    Processed_Lab_svar = pd.concat([Processed_Lab_svar_1,Processed_Lab_svar_2], ignore_index = True)\n",
    "\n",
    "    Processed_Lab_svar = Processed_Lab_svar.applymap(lambda x: 1 if x == True else (0 if x == False else x))\n",
    "    # print(\"Processed_Lab_svar:\")\n",
    "    # print(Processed_Lab_svar)\n",
    "\n",
    "    # Step 2: Add missing IDno from Populationen with all other columns filled with 0\n",
    "    missing_ids = IDS_dfx.loc[~IDS_dfx['IDno'].isin(Processed_Lab_svar['IDno']), 'IDno']\n",
    "    # print(\"Missing ids:\")\n",
    "    # print(missing_ids)\n",
    "    missing_data = pd.DataFrame({'IDno': missing_ids})\n",
    "    # print(\"Missing data:\")\n",
    "    # print(missing_data)\n",
    "    columns_to_add = [col for col in Processed_Lab_svar.columns if col != 'IDno']\n",
    "    missing_data[columns_to_add] = 0\n",
    "\n",
    "    Processed_Lab_svar = pd.concat([Processed_Lab_svar, missing_data], ignore_index=True)\n",
    "    Processed_Lab_svar = Processed_Lab_svar.groupby('IDno').max().reset_index()\n",
    "    pre_liste.append(Processed_Lab_svar)\n",
    "\n",
    "    return Processed_Lab_svar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Lab_svar_1_Phase_1, Processed_Lab_svar_2_Phase_1, kol_navn_Phase_1,ny_list_Phase_1 = Lab_svarProcessing1(Lab_svar_1_Phase_1,Lab_svar_2_Phase_1)\n",
    "Processed_Lab_svar_1_Phase_2, Processed_Lab_svar_2_Phase_2, kol_navn_Phase_2,ny_list_Phase_2 = Lab_svarProcessing1(Lab_svar_1_Phase_2,Lab_svar_2_Phase_2)\n",
    "Processed_Lab_svar_1_Phase_3, Processed_Lab_svar_2_Phase_3, kol_navn_Phase_3,ny_list_Phase_3 = Lab_svarProcessing1(Lab_svar_1_Phase_3,Lab_svar_2_Phase_3)\n",
    "Processed_Lab_svar_1_Phase_4, Processed_Lab_svar_2_Phase_4, kol_navn_Phase_4,ny_list_Phase_4 = Lab_svarProcessing1(Lab_svar_1_Phase_4,Lab_svar_2_Phase_4)\n",
    "Processed_Lab_svar_1_Phase_5, Processed_Lab_svar_2_Phase_5, kol_navn_Phase_5,ny_list_Phase_5 = Lab_svarProcessing1(Lab_svar_1_Phase_5,Lab_svar_2_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Lab_svar_1_Phase_1, Processed_Lab_svar_2_Phase_1, hvilke_Phase_1, antal_Phase_1 = Lab_svarProcessing2(Processed_Lab_svar_1_Phase_1, Processed_Lab_svar_2_Phase_1, kol_navn_Phase_1,ny_list_Phase_1)\n",
    "Processed_Lab_svar_1_Phase_2, Processed_Lab_svar_2_Phase_2, hvilke_Phase_2, antal_Phase_2 = Lab_svarProcessing2(Processed_Lab_svar_1_Phase_2, Processed_Lab_svar_2_Phase_2, kol_navn_Phase_2,ny_list_Phase_2)\n",
    "Processed_Lab_svar_1_Phase_3, Processed_Lab_svar_2_Phase_3, hvilke_Phase_3, antal_Phase_3 = Lab_svarProcessing2(Processed_Lab_svar_1_Phase_3, Processed_Lab_svar_2_Phase_3, kol_navn_Phase_3,ny_list_Phase_3)\n",
    "Processed_Lab_svar_1_Phase_4, Processed_Lab_svar_2_Phase_4, hvilke_Phase_4, antal_Phase_4 = Lab_svarProcessing2(Processed_Lab_svar_1_Phase_4, Processed_Lab_svar_2_Phase_4, kol_navn_Phase_4,ny_list_Phase_4)\n",
    "Processed_Lab_svar_1_Phase_5, Processed_Lab_svar_2_Phase_5, hvilke_Phase_5, antal_Phase_5 = Lab_svarProcessing2(Processed_Lab_svar_1_Phase_5, Processed_Lab_svar_2_Phase_5, kol_navn_Phase_5,ny_list_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Lab_svar_Phase_1 = Lab_svarProcessing3(Processed_Lab_svar_1_Phase_1, Processed_Lab_svar_2_Phase_1)\n",
    "Processed_Lab_svar_Phase_2 = Lab_svarProcessing3(Processed_Lab_svar_1_Phase_2, Processed_Lab_svar_2_Phase_2)\n",
    "Processed_Lab_svar_Phase_3 = Lab_svarProcessing3(Processed_Lab_svar_1_Phase_3, Processed_Lab_svar_2_Phase_3)\n",
    "Processed_Lab_svar_Phase_4 = Lab_svarProcessing3(Processed_Lab_svar_1_Phase_4, Processed_Lab_svar_2_Phase_4)\n",
    "Processed_Lab_svar_Phase_5 = Lab_svarProcessing3(Processed_Lab_svar_1_Phase_5, Processed_Lab_svar_2_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Lab_svar_Phase_1.to_csv(\"Phases/Processed_Lab_svar_Phase_1.csv\", index=False)\n",
    "Processed_Lab_svar_Phase_2.to_csv(\"Phases/Processed_Lab_svar_Phase_2.csv\", index=False)\n",
    "Processed_Lab_svar_Phase_3.to_csv(\"Phases/Processed_Lab_svar_Phase_3.csv\", index=False)\n",
    "Processed_Lab_svar_Phase_4.to_csv(\"Phases/Processed_Lab_svar_Phase_4.csv\", index=False)\n",
    "Processed_Lab_svar_Phase_5.to_csv(\"Phases/Processed_Lab_svar_Phase_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skal ændres: AHM, Anæstesidata, Blødning, Total indgift, \n",
    "Skal indkluderes: Dialyse, Labsvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHM Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AHMProcessing(AHM):\n",
    "    # Kolonner, der har mindre end 1000 målinger i hele AHM\n",
    "    columns_to_drop = ['Timestamp', 'ABP inv BT', 'AO Puls (fra A-kanyle)', 'AO inv BT', 'AO mean inv BT', 'ART Puls (fra A-kanyle)', 'ART inv BT', 'Art', 'Blodtryk', 'CCI', 'CCO', 'CI', 'CO', 'CPP', 'CVP (sys/dia)',\n",
    "    'EVLW', 'EVLWi', 'GEDV', 'GEDVi', 'Invasivt BT - ABP (sys/dia)', 'Invasivt BT - AO (middel)', 'Invasivt BT - AO (sys/dia)', 'Invasivt BT - ART (mean)', 'Invasivt BT - ART (sys/dia)', 'Invasivt BT - UAP (middel)',\n",
    "    'Invasivt BT - UAP (sys/dia)', 'LAP', 'LAP (middel)', 'LAP (sys/dia)', 'NIBP', 'PAP', 'PAP (sys/dia)', 'PCWP', 'PPV', 'PVR', 'PVRI', 'Puls (fra A-kanyle - ART)', 'Puls 2 (fra A-kanyle - UAP)', 'Puls amplitude',\n",
    "    'RAP (middel)', 'RAP (sys/dia)', 'SATvO2', 'SVR', 'SVRI', 'SVi', 'Saturation, fod (postduktal)', 'Saturation, højre arm (præduktal)', 'Slagvolumen (SV)', 'Slagvolumenvariation (SVV)', 'Slagvolumenvariation LIDCO',\n",
    "    'Sondedybde (cm)', 'SpO2 L', 'SpO2 R', 'SvO2', 'Temperatur 1', 'Temperatur 2', 'Temperatur 3', 'Temperatur 4', 'Temperatur, højre', 'Temperatur, venstre','RF - indstillet']\n",
    "    # RF - indstillet fjernet fordi den er det samme for alle\n",
    "    Beginning_AHM = AHM.drop(columns=columns_to_drop)\n",
    "    \n",
    "    remove_more_columns = []\n",
    "    non_null_counts = Beginning_AHM.groupby('IDno').count()\n",
    "\n",
    "    # Filter columns with all null values for each group\n",
    "    columns_with_all_nulls = non_null_counts.columns[(non_null_counts == 0).all()].tolist()\n",
    "    non_null_counts_no_zeros = non_null_counts.replace(0, np.nan)\n",
    "    count_AHM_pr_ID = non_null_counts_no_zeros.count()\n",
    "\n",
    "    for column, amounts in count_AHM_pr_ID.items():\n",
    "        if amounts/len(Beginning_AHM['IDno'].unique())*100 < 80: # Måske en anden værdi\n",
    "            remove_more_columns.append(column)\n",
    "    \n",
    "    if len(remove_more_columns)>0:\n",
    "        Beginning_AHM = Beginning_AHM.drop(columns=remove_more_columns)\n",
    "\n",
    "    dummies = pd.get_dummies(Beginning_AHM['Skema navn']).astype(int)\n",
    "\n",
    "    skema_titler = list(dummies.columns)\n",
    "\n",
    "    Beginning_AHM = pd.concat([Beginning_AHM, dummies], axis = 1)\n",
    "\n",
    "    Beginning_AHM = Beginning_AHM.drop(columns = 'Skema navn')\n",
    "\n",
    "    original_columns = [col for col in Beginning_AHM.columns if col not in ['IDno'] + skema_titler]\n",
    "\n",
    "    grouped = Beginning_AHM.groupby('IDno')[original_columns].agg(['mean', 'var'])\n",
    "\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "    Processed_AHM = grouped.reset_index()    \n",
    "\n",
    "    dummies_grouped = Beginning_AHM.groupby('IDno')[list(dummies.columns)].max().reset_index()\n",
    "\n",
    "    Processed_AHM = pd.merge(Processed_AHM,dummies_grouped, on='IDno')\n",
    "\n",
    "    missing_idnos = IDS_dfx[~IDS_dfx['IDno'].isin(Processed_AHM['IDno'])]['IDno']\n",
    "\n",
    "    # Create a dataframe with these missing IDnos and 0 in every other input\n",
    "    missing_data = pd.DataFrame(missing_idnos, columns=['IDno'])\n",
    "    for col in Processed_AHM.columns:\n",
    "        if col != 'IDno':\n",
    "            missing_data[col] = 0\n",
    "\n",
    "    # Append this missing data to Processed_AHM\n",
    "    Processed_AHM = pd.concat([Processed_AHM, missing_data], ignore_index=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    Processed_AHM.fillna(0, inplace=True)\n",
    "    Processed_AHM = Processed_AHM.groupby('IDno').max().reset_index()\n",
    "    \n",
    "    return Processed_AHM        \n",
    "        \n",
    "    # Start med at tjekke om alle værdierne i en kolonne er NaN, if so kan den nok godt droppes\n",
    "    # Ved ikke, hvordan skema navn skal håndteres, skriv en mail til Theis og Lars\n",
    "    # Der er nogle kolonner, der kun har meget få målinger, kan evt. være at de skal undlades. Hver fase har forskellige mængder rækker, så måske en procentdel? Findes med statistik? Fase 1 har meget få målinger, de andre er bedre\n",
    "    # På dette tidspunkt er vi nede på maks 27 kolonner, som der nok skal bestemmes mean og varians af. Måske ok mængde? Tror, det er færre end Lab svar\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_AHM_Phase_1 = AHMProcessing(AHM_Phase_1)\n",
    "Processed_AHM_Phase_2 = AHMProcessing(AHM_Phase_2)\n",
    "Processed_AHM_Phase_3 = AHMProcessing(AHM_Phase_3)\n",
    "Processed_AHM_Phase_4 = AHMProcessing(AHM_Phase_4)\n",
    "Processed_AHM_Phase_5 = AHMProcessing(AHM_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_AHM_Phase_1.to_csv(\"Processed_AHM_Phase_1.csv\", index=False)\n",
    "Processed_AHM_Phase_2.to_csv(\"Processed_AHM_Phase_2.csv\", index=False)\n",
    "Processed_AHM_Phase_3.to_csv(\"Processed_AHM_Phase_3.csv\", index=False)\n",
    "Processed_AHM_Phase_4.to_csv(\"Processed_AHM_Phase_4.csv\", index=False)\n",
    "Processed_AHM_Phase_5.to_csv(\"Processed_AHM_Phase_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_null_counts[non_null_counts > 1000].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = AHM.groupby('IDno').count()\n",
    "\n",
    "# Filter columns with all null values for each group\n",
    "columns_with_all_nulls = non_null_counts.columns[(non_null_counts == 0).all()].tolist()\n",
    "non_null_counts_no_zeros = non_null_counts.replace(0, np.nan)\n",
    "count_AHM_pr_ID = non_null_counts_no_zeros.count()\n",
    "\n",
    "# Set pandas display options to show all columns\n",
    "# pd.set_option('display.max_columns', None)  # To display all columns\n",
    "# pd.set_option('display.max_rows', None)  # To display all rows if needed\n",
    "\n",
    "# Print the non-zero counts\n",
    "print(\"Count of non-zero values in each column:\")\n",
    "print(count_AHM_pr_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_AHM_pr_ID[count_AHM_pr_ID < 1000].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolonner, der har mindre end 1000 målinger i hele AHM\n",
    "columns_to_drop = ['Timestamp', 'ABP inv BT', 'AO Puls (fra A-kanyle)', 'AO inv BT', 'AO mean inv BT', 'ART Puls (fra A-kanyle)', 'ART inv BT', 'Art', 'Blodtryk', 'CCI', 'CCO', 'CI', 'CO', 'CPP', 'CVP (sys/dia)',\n",
    "    'EVLW', 'EVLWi', 'GEDV', 'GEDVi', 'Invasivt BT - ABP (sys/dia)', 'Invasivt BT - AO (middel)', 'Invasivt BT - AO (sys/dia)', 'Invasivt BT - ART (mean)', 'Invasivt BT - ART (sys/dia)', 'Invasivt BT - UAP (middel)',\n",
    "    'Invasivt BT - UAP (sys/dia)', 'LAP', 'LAP (middel)', 'LAP (sys/dia)', 'NIBP', 'PAP', 'PAP (sys/dia)', 'PCWP', 'PPV', 'PVR', 'PVRI', 'Puls (fra A-kanyle - ART)', 'Puls 2 (fra A-kanyle - UAP)', 'Puls amplitude',\n",
    "    'RAP (middel)', 'RAP (sys/dia)', 'SATvO2', 'SVR', 'SVRI', 'SVi', 'Saturation, fod (postduktal)', 'Saturation, højre arm (præduktal)', 'Slagvolumen (SV)', 'Slagvolumenvariation (SVV)', 'Slagvolumenvariation LIDCO',\n",
    "    'Sondedybde (cm)', 'SpO2 L', 'SpO2 R', 'SvO2', 'Temperatur 1', 'Temperatur 2', 'Temperatur 3', 'Temperatur 4', 'Temperatur, højre', 'Temperatur, venstre']\n",
    "len(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_cleaned = AHM.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_cleaned_Phase_1 = AHM_Phase_1.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHM_cleaned_Phase_2 = AHM_Phase_2.drop(columns=columns_to_drop)\n",
    "AHM_cleaned_Phase_3 = AHM_Phase_3.drop(columns=columns_to_drop)\n",
    "AHM_cleaned_Phase_4 = AHM_Phase_4.drop(columns=columns_to_drop)\n",
    "AHM_cleaned_Phase_5 = AHM_Phase_5.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = AHM_cleaned_Phase_5.groupby('IDno').count()\n",
    "\n",
    "# Filter columns with all null values for each group\n",
    "columns_with_all_nulls = non_null_counts.columns[(non_null_counts == 0).all()].tolist()\n",
    "non_null_counts_no_zeros = non_null_counts.replace(0, np.nan)\n",
    "count_AHM_pr_ID = non_null_counts_no_zeros.count()\n",
    "\n",
    "# Set pandas display options to show all columns\n",
    "# pd.set_option('display.max_columns', None)  # To display all columns\n",
    "# pd.set_option('display.max_rows', None)  # To display all rows if needed\n",
    "\n",
    "# Print the non-zero counts\n",
    "# print(\"Count of non-zero values in each column:\")\n",
    "# print(count_AHM_pr_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_more_columns = []\n",
    "\n",
    "for column, amounts in (count_AHM_pr_ID).items():\n",
    "    print(column)\n",
    "    print(amounts/len(AHM_cleaned_Phase_5['IDno'].unique())*100) # Måske en anden værdi\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anæstesidata Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anæstesidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnæstesidataProcessing(Anæstesidata):\n",
    "    Beginning_Anæstesidata = Anæstesidata\n",
    "    Beginning_Anæstesidata[['ABP inv BT Systolisk','ABP inv BT Diastolisk']] = Beginning_Anæstesidata['ABP inv BT'].str.split('/', expand = True).astype(float)\n",
    "    \n",
    "    remove_more_columns = ['Timestamp','ABP inv BT']\n",
    "    non_null_counts = Beginning_Anæstesidata.groupby('IDno').count()\n",
    "\n",
    "    # Filter columns with all null values for each group\n",
    "    columns_with_all_nulls = non_null_counts.columns[(non_null_counts == 0).all()].tolist()\n",
    "    non_null_counts_no_zeros = non_null_counts.replace(0, np.nan)\n",
    "    count_AHM_pr_ID = non_null_counts_no_zeros.count()\n",
    "\n",
    "    for column, amounts in count_AHM_pr_ID.items():\n",
    "        if amounts/len(Beginning_Anæstesidata['IDno'].unique())*100 < 80: # Måske en anden værdi\n",
    "            remove_more_columns.append(column)\n",
    "    \n",
    "    if len(remove_more_columns)>0:\n",
    "        Beginning_Anæstesidata = Beginning_Anæstesidata.drop(columns=remove_more_columns)\n",
    "\n",
    "    # List of columns to calculate mean and variance\n",
    "    columns_to_aggregate = [col for col in Beginning_Anæstesidata.columns if col not in ['IDno']]\n",
    "\n",
    "    # Calculate mean and variance grouped by 'IDno'\n",
    "    grouped = Beginning_Anæstesidata.groupby('IDno')[columns_to_aggregate].agg(['mean', 'var'])\n",
    "    \n",
    "    # Flatten the hierarchical column index\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "    # Reset index to bring 'IDno' back as a column\n",
    "    Processed_Anæstesidata = grouped.reset_index()\n",
    "    \n",
    "    missing_idnos = IDS_dfx[~IDS_dfx['IDno'].isin(Processed_Anæstesidata['IDno'])]['IDno']\n",
    "\n",
    "    # Create a dataframe with these missing IDnos and 0 in every other input\n",
    "    missing_data = pd.DataFrame(missing_idnos, columns=['IDno'])\n",
    "    for col in Processed_Anæstesidata.columns:\n",
    "        if col != 'IDno':\n",
    "            missing_data[col] = 0\n",
    "\n",
    "    # Append this missing data to Processed_Anæstesidata\n",
    "    Processed_Anæstesidata = pd.concat([Processed_Anæstesidata, missing_data], ignore_index=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    Processed_Anæstesidata.fillna(0, inplace=True)\n",
    "    Processed_Anæstesidata = Processed_Anæstesidata.groupby('IDno').max().reset_index()\n",
    "    \n",
    "    return Processed_Anæstesidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Anæstesidata_Phase_1 = AnæstesidataProcessing(Anæstesidata_Phase_1)\n",
    "Processed_Anæstesidata_Phase_2 = AnæstesidataProcessing(Anæstesidata_Phase_2)\n",
    "Processed_Anæstesidata_Phase_3 = AnæstesidataProcessing(Anæstesidata_Phase_3)\n",
    "Processed_Anæstesidata_Phase_4 = AnæstesidataProcessing(Anæstesidata_Phase_4)\n",
    "Processed_Anæstesidata_Phase_5 = AnæstesidataProcessing(Anæstesidata_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Anæstesidata_Phase_1.to_csv(\"Phases/Processed_Anæstesidata_Phase_1.csv\", index=False)\n",
    "Processed_Anæstesidata_Phase_2.to_csv(\"Phases/Processed_Anæstesidata_Phase_2.csv\", index=False)\n",
    "Processed_Anæstesidata_Phase_3.to_csv(\"Phases/Processed_Anæstesidata_Phase_3.csv\", index=False)\n",
    "Processed_Anæstesidata_Phase_4.to_csv(\"Phases/Processed_Anæstesidata_Phase_4.csv\", index=False)\n",
    "Processed_Anæstesidata_Phase_5.to_csv(\"Phases/Processed_Anæstesidata_Phase_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blødning Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BlødningProcessing(Blødning):\n",
    "    Beginning_Blødning = Blødning\n",
    "    \n",
    "    remove_columns = ['Timestamp']\n",
    "\n",
    "    Beginning_Blødning = Beginning_Blødning.drop(columns = remove_columns)\n",
    "\n",
    "    # List of columns to calculate mean and variance\n",
    "    columns_to_aggregate = [col for col in Beginning_Blødning.columns if col not in ['IDno']]\n",
    "\n",
    "    # Calculate mean and variance grouped by 'IDno'\n",
    "    grouped = Beginning_Blødning.groupby('IDno')[columns_to_aggregate].agg(['mean', 'var'])\n",
    "    \n",
    "    # Flatten the hierarchical column index\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "    # Reset index to bring 'IDno' back as a column\n",
    "    Processed_Blødning = grouped.reset_index()\n",
    "    \n",
    "    missing_idnos = IDS_dfx[~IDS_dfx['IDno'].isin(Processed_Blødning['IDno'])]['IDno']\n",
    "\n",
    "    # Create a dataframe with these missing IDnos and 0 in every other input\n",
    "    missing_data = pd.DataFrame(missing_idnos, columns=['IDno'])\n",
    "    for col in Processed_Blødning.columns:\n",
    "        if col != 'IDno':\n",
    "            missing_data[col] = 0\n",
    "\n",
    "    # Append this missing data to Processed_Blødning\n",
    "    Processed_Blødning = pd.concat([Processed_Blødning, missing_data], ignore_index=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    Processed_Blødning.fillna(0, inplace=True)\n",
    "    Processed_Blødning = Processed_Blødning.groupby('IDno').max().reset_index()\n",
    "    \n",
    "    return Processed_Blødning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Blødning_Phase_1 = BlødningProcessing(Blødning_Phase_1)\n",
    "Processed_Blødning_Phase_2 = BlødningProcessing(Blødning_Phase_2)\n",
    "Processed_Blødning_Phase_3 = BlødningProcessing(Blødning_Phase_3)\n",
    "Processed_Blødning_Phase_4 = BlødningProcessing(Blødning_Phase_4)\n",
    "Processed_Blødning_Phase_5 = BlødningProcessing(Blødning_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Blødning_Phase_1.to_csv(\"Phases/Processed_Blødning_Phase_1.csv\", index=False)\n",
    "Processed_Blødning_Phase_2.to_csv(\"Phases/Processed_Blødning_Phase_2.csv\", index=False)\n",
    "Processed_Blødning_Phase_3.to_csv(\"Phases/Processed_Blødning_Phase_3.csv\", index=False)\n",
    "Processed_Blødning_Phase_4.to_csv(\"Phases/Processed_Blødning_Phase_4.csv\", index=False)\n",
    "Processed_Blødning_Phase_5.to_csv(\"Phases/Processed_Blødning_Phase_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blødning.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blødning_Phase_1['Blødning ml'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total indgift Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_indgift.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_indgiftProcessing(Total_indgift):\n",
    "    Beginning_Total_indgift = Total_indgift\n",
    "\n",
    "    Beginning_Total_indgift = Beginning_Total_indgift.drop(columns = ['Kommentar','Timestamp'])\n",
    "\n",
    "    dummies1 = pd.get_dummies(Beginning_Total_indgift['Ordineret lægemiddel navn']).astype(int)\n",
    "\n",
    "    dummies2 = pd.get_dummies(Beginning_Total_indgift['Række navn']).astype(int)\n",
    "\n",
    "    skema_titler1 = list(dummies1.columns)\n",
    "\n",
    "    skema_titler2 = list(dummies2.columns)\n",
    "\n",
    "    Beginning_Total_indgift = pd.concat([Beginning_Total_indgift, dummies1, dummies2], axis = 1)\n",
    "\n",
    "    Beginning_Total_indgift = Beginning_Total_indgift.drop(columns = ['Ordineret lægemiddel navn', 'Række navn'])\n",
    "\n",
    "    original_columns = [col for col in Beginning_Total_indgift.columns if col not in ['IDno'] + skema_titler1 + skema_titler2]\n",
    "\n",
    "    grouped = Beginning_Total_indgift.groupby('IDno')[original_columns].agg(['mean', 'var'])\n",
    "\n",
    "    grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "\n",
    "    Processed_Total_indgift = grouped.reset_index()    \n",
    "\n",
    "    dummies_grouped1 = Beginning_Total_indgift.groupby('IDno')[list(dummies1.columns)].max().reset_index()\n",
    "\n",
    "    dummies_grouped2 = Beginning_Total_indgift.groupby('IDno')[list(dummies2.columns)].max().reset_index()\n",
    "\n",
    "    Processed_Total_indgift = pd.merge(Processed_Total_indgift,dummies_grouped1, on='IDno')\n",
    "\n",
    "    Processed_Total_indgift = pd.merge(Processed_Total_indgift,dummies_grouped2, on='IDno')\n",
    "\n",
    "    missing_idnos = IDS_dfx[~IDS_dfx['IDno'].isin(Processed_Total_indgift['IDno'])]['IDno']\n",
    "\n",
    "    # Create a dataframe with these missing IDnos and 0 in every other input\n",
    "    missing_data = pd.DataFrame(missing_idnos, columns=['IDno'])\n",
    "    for col in Processed_Total_indgift.columns:\n",
    "        if col != 'IDno':\n",
    "            missing_data[col] = 0\n",
    "\n",
    "    # Append this missing data to Processed_Total_indgift\n",
    "    Processed_Total_indgift = pd.concat([Processed_Total_indgift, missing_data], ignore_index=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    Processed_Total_indgift.fillna(0, inplace=True)\n",
    "    Processed_Total_indgift = Processed_Total_indgift.groupby('IDno').max().reset_index()\n",
    "    \n",
    "    return Processed_Total_indgift        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Total_indgift_Phase_1 = Total_indgiftProcessing(Total_indgift_Phase_1)\n",
    "Processed_Total_indgift_Phase_2 = Total_indgiftProcessing(Total_indgift_Phase_2)\n",
    "Processed_Total_indgift_Phase_3 = Total_indgiftProcessing(Total_indgift_Phase_3)\n",
    "Processed_Total_indgift_Phase_4 = Total_indgiftProcessing(Total_indgift_Phase_4)\n",
    "Processed_Total_indgift_Phase_5 = Total_indgiftProcessing(Total_indgift_Phase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Total_indgift_Phase_1.to_csv(\"Phases/Processed_Total_indgift_Phase_1.csv\", index=False)\n",
    "Processed_Total_indgift_Phase_2.to_csv(\"Phases/Processed_Total_indgift_Phase_2.csv\", index=False)\n",
    "Processed_Total_indgift_Phase_3.to_csv(\"Phases/Processed_Total_indgift_Phase_3.csv\", index=False)\n",
    "Processed_Total_indgift_Phase_4.to_csv(\"Phases/Processed_Total_indgift_Phase_4.csv\", index=False)\n",
    "Processed_Total_indgift_Phase_5.to_csv(\"Phases/Processed_Total_indgift_Phase_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Processed_Total_indgift_Phase_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Total_indgift_Phase_1['Ordineret lægemiddel navn'].unique()),len(Total_indgift_Phase_1['Række navn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Total_indgift_Phase_2['Ordineret lægemiddel navn'].unique()),len(Total_indgift_Phase_2['Række navn'].unique()))\n",
    "print(len(Total_indgift_Phase_3['Ordineret lægemiddel navn'].unique()),len(Total_indgift_Phase_3['Række navn'].unique()))\n",
    "print(len(Total_indgift_Phase_4['Ordineret lægemiddel navn'].unique()),len(Total_indgift_Phase_4['Række navn'].unique()))\n",
    "print(len(Total_indgift_Phase_5['Ordineret lægemiddel navn'].unique()),len(Total_indgift_Phase_5['Række navn'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Total_indgift.copy()\n",
    "test = test.fillna('')\n",
    "test['Ordineret og Rækken navn'] = test['Række navn']+test['Ordineret lægemiddel navn']\n",
    "len(test['Ordineret og Rækken navn'].unique()),len(test['Ordineret lægemiddel navn'].unique()),len(test['Række navn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_more_columns = []\n",
    "non_null_counts = Total_indgift_Phase_1.groupby('IDno').count()\n",
    "\n",
    "# Filter columns with all null values for each group\n",
    "columns_with_all_nulls = non_null_counts.columns[(non_null_counts == 0).all()].tolist()\n",
    "non_null_counts_no_zeros = non_null_counts.replace(0, np.nan)\n",
    "count_AHM_pr_ID = non_null_counts_no_zeros.count()\n",
    "\n",
    "for column, amounts in count_AHM_pr_ID.items():\n",
    "    if amounts/len(Total_indgift_Phase_1['IDno'].unique())*100 < 80: # Måske en anden værdi\n",
    "        remove_more_columns.append(column)\n",
    "        print(amounts)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_indgift_Phase_1[Total_indgift_Phase_1['Ordineret lægemiddel navn'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "liste = []\n",
    "for item in sorted(list(Total_indgift_Phase_1[Total_indgift_Phase_1['Ordineret lægemiddel navn'].isna()]['Værdi'])):\n",
    "    if item < 1:\n",
    "        count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_indgift_Phase_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X matricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "dataframes_to_merge = [Processed_AHM_Phase_1, Processed_Anæstesidata_Phase_1, Processed_Lab_svar_Phase_1, Processed_Total_indgift_Phase_1]\n",
    "\n",
    "x_matrix_phase_1 = reduce(lambda left, right: pd.merge(left, right, on='IDno', how='outer'), dataframes_to_merge)\n",
    "x_matrix_phase_1 = x_matrix_phase_1.replace('Kvinde',0)\n",
    "x_matrix_phase_1 = x_matrix_phase_1.replace('Mand',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_merge = [Processed_AHM_Phase_2, Processed_Anæstesidata_Phase_2, Processed_Lab_svar_Phase_2, Processed_Total_indgift_Phase_2]\n",
    "\n",
    "x_matrix_phase_2 = reduce(lambda left, right: pd.merge(left, right, on='IDno', how='outer'), dataframes_to_merge)\n",
    "x_matrix_phase_2 = x_matrix_phase_2.replace('Kvinde',0)\n",
    "x_matrix_phase_2 = x_matrix_phase_2.replace('Mand',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_merge = [Processed_AHM_Phase_3, Processed_Anæstesidata_Phase_3, Processed_Lab_svar_Phase_3, Processed_Total_indgift_Phase_3]\n",
    "\n",
    "x_matrix_phase_3 = reduce(lambda left, right: pd.merge(left, right, on='IDno', how='outer'), dataframes_to_merge)\n",
    "x_matrix_phase_3 = x_matrix_phase_3.replace('Kvinde',0)\n",
    "x_matrix_phase_3 = x_matrix_phase_3.replace('Mand',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_merge = [Processed_AHM_Phase_4, Processed_Anæstesidata_Phase_4, Processed_Lab_svar_Phase_4, Processed_Total_indgift_Phase_4]\n",
    "\n",
    "x_matrix_phase_4 = reduce(lambda left, right: pd.merge(left, right, on='IDno', how='outer'), dataframes_to_merge)\n",
    "x_matrix_phase_4 = x_matrix_phase_4.replace('Kvinde',0)\n",
    "x_matrix_phase_4 = x_matrix_phase_4.replace('Mand',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_merge = [Processed_AHM_Phase_5, Processed_Anæstesidata_Phase_5, Processed_Lab_svar_Phase_5, Processed_Total_indgift_Phase_5]\n",
    "\n",
    "x_matrix_phase_5 = reduce(lambda left, right: pd.merge(left, right, on='IDno', how='outer'), dataframes_to_merge)\n",
    "x_matrix_phase_5 = x_matrix_phase_5.replace('Kvinde',0)\n",
    "x_matrix_phase_5 = x_matrix_phase_5.replace('Mand',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix_phase_1.to_csv('x_matrix_phase_1_4738.csv',compression='gzip',index=False)\n",
    "x_matrix_phase_2.to_csv('x_matrix_phase_2_4738.csv',compression='gzip',index=False)\n",
    "x_matrix_phase_3.to_csv('x_matrix_phase_3_4738.csv',compression='gzip',index=False)\n",
    "x_matrix_phase_4.to_csv('x_matrix_phase_4_4738.csv',compression='gzip',index=False)\n",
    "x_matrix_phase_5.to_csv('x_matrix_phase_5_4738.csv',compression='gzip',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
